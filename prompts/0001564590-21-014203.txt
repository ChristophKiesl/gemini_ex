Prompt: Summarize the business model from the following text. Answer with a continuous text and with fivehundredtwelve tokens at max. Set your focus on sources of revenue , the intended customer base , products , distribution channels  and details of financing. Use only information from the following the text:

Item 1.  Business
Overview
 MoSys, Inc., together with its subsidiaries (“MoSys,” the “Company,” “we,” “our” or “us”), is a fabless semiconductor company focused on the development and sale of integrated circuits, or ICs, for the high-speed cloud networking, communications, security appliance, video, monitor and test, data center and computing markets. Our solutions deliver time-to-market, performance, power, area and economic benefits for system original equipment manufacturers, or OEMs. Our primary product line is marketed under the Accelerator Engine name and comprises our Bandwidth Engine and Programmable HyperSpeed Engine IC products, which integrate our proprietary, 1T-SRAM high-density embedded memory and a highly-efficient serial interface protocol resulting in a monolithic memory IC solution optimized for memory bandwidth and transaction access performance.   Further performance benefits can be achieved to offload statistical, search or other custom functions using our optional integrated logic and processor elements.
   As data rates and the amount of high-speed processing increase, critical memory access bottlenecks occur. Our Accelerator Engine ICs dramatically increase memory accesses per second, removing these bottlenecks. In addition, the serial interface and high-memory capacity reduce the board footprint, number of pins and complexity, while using less power. To complement our Accelerator Engine ICs and utilize our technology we have been developing our Virtual Accelerator Engine, or VAE, product line that leverages our proprietary graph memory engine technology to provide data classification capabilities through the use of high-speed memories.   Our VAE products include software, firmware and related intellectual property, or IP, and are hardware agnostic and operate with or without one of our Accelerator Engine IC products. 
 3
 
Our LineSpeed IC product line comprises non-memory, high-speed serialization-deserialization interface, or SerDes I/O, physical layer, or PHY, devices that ensure signal integrity between interfaces which is commonly referred to as clock data recovery, or CDR, or retimer functionality, which perform multiplexing to transition from one speed to another, commonly referred to as Gearbox functionality. These PHY devices reside within optical modules and on networking equipment line cards designed for next-generation Ethernet and optical transport network applications.
 Industry Background
 The amount of data and the number of data consumers and devices continues to grow, driven primarily by commercial and consumer cloud applications, video services, high speed mobile networks, Internet of Things, or IoT, and many other cloud applications. In order to meet these demands, the new cloud infrastructure, including the backbone, edge, access network and data centers, must scale in both speed and intelligence to handle real-time security, bandwidth allocation, and service-level expectations. In addition, workloads or applications delivered at a massive scale from the cloud require flexible and efficient data transmission to optimize resources to enable these applications and lower the overall cost, size and power of the data center. These increased demands strain communication between onboard IC devices, limiting the data throughput in network switches and routers and the network backbone.
 To meet these demands, carrier and enterprise networks are merging with the cloud and are undergoing significant changes and, most significantly, are migrating to packet-based Ethernet networks that enable higher throughput, lower cost and uniform technology across access, core and metro network infrastructure. These networks have been designed to deliver voice and video applications over high-speed Internet services on one converged, efficient and flexible network. These trends require networking systems, especially the high-speed switches, security appliances and routers that primarily comprise these networks, to comply with evolving market requirements and be capable of providing new services and better quality of service while supporting new protocols and standards. Traditional OEM network and telecommunications equipment manufacturers, such as Nokia Corporation, and its subsidiary, Alcatel-Lucent, Cisco Systems, Inc., Tel. LM Ericsson, Fujitsu Ltd., Hitachi Ltd., Huawei Technologies, and Juniper Networks, Inc., as well as new vendors and cloud-service providers, who are delivering a new set of white-box solutions, must offer higher levels of packet forwarding rates, bandwidth density and be optimized to enable higher-density, lower-power data path connectivity in the next generations of their networking systems.
 Networking communications, security, video and computing systems throughout the cloud network must operate at higher speed and performance levels, and so require new generations of packet processors and improved memory subsystems to enable system performance. These systems and their component line cards generally need to support aggregate rates of 100 gigabits per second, or Gbps, and above to meet the continued growth in network traffic. Data centers and access equipment that were previously aggregating slower traffic at rates of up to 40Gbps now are being designed to aggregate traffic at 100Gbps, or more. The transition to high-bandwidth networks and the move to 100Gbps and higher rates at the edge (i.e., closer to the networks and users generating data, voice and video traffic) is underway, and the increase in data rates for these networks is expected to continue to grow rapidly over the coming years. 
 The systems that our customers build come in various sizes and utilize cards that contain several types of semiconductors. Line cards are found in chassis-based systems that have slots and can contain up to 20 line cards. Our networking and communication system and certain other system customers typically use chassis-based systems. The alternative is systems that contain a single card; these systems are generally referred to as appliances or “pizza boxes.”  Cards that typically plug into a server or compute system are generally referred to as accelerator cards. We believe the wider use of these accelerator cards in systems throughout the network, especially at the edge, will expand the market opportunity for our products, as a number of these cards utilize field programmable gate array ICs, or FPGAs, as the packet processor. Our Accelerator Engines are ideally suited to support FPGAs performing these functions.  Each line card, or accelerator card, includes one or more processors and multiple memory chips. These processors are complex ICs or IC chipsets that perform high-speed data or packet processing for functions, such as traffic routing, shaping, metering, billing, statistics, detection, steering, security, video processing, monitoring and workload acceleration. The line cards use various types of memory ICs to facilitate temporary packet storage and assist in the analysis and tracking of information embedded within the data flowing through the processors. After a packet enters the line card, a packet or data processor helps separate the packet into smaller pieces for rapid analysis. In a typical packet-based network for example, the data is broken up into the packet 
 4
 
header, which contains vital information on packet destination and type, such as the Internet protocol address, and the payload, which contains the data being sent. Generally, the line card operations must occur at full data rates and typically require frequent access to the memory ICs.
 Simultaneously, the packet’s payload, which may be substantially larger than the packet header, is also stored in memory ICs until processing is complete and the packet can be re-combined and sent to its next system destination. Within the line card, communication between the packet processor and memory ICs occurs through an interface consisting of combinations of physical pins on each type of chip. These pins are grouped together in a parallel or a serial architecture to form a pathway, called a bus, through which information is transferred from one IC to the next.
 Today, the majority of physical buses that connect networking equipment and components use a parallel architecture to communicate between processors and memory ICs, which means information can travel only in one direction and in one instance at a time. As processing speeds increase, the number of pins required and the speed of the bus in a parallel architecture become a limitation on system performance and capability. In contrast, the number of connections is reduced substantially across fewer, higher-rate pins in a serial architecture, and data is transferred simultaneously in both directions. Data transfer rates are limited by the data access rates of the various ICs included on the line card, thus leading to bottlenecks when these ICs perform inadequately. In order to remove these bottlenecks and meet next-generation bandwidth requirements, the line card ICs need to support higher access rates enabled by internal memory or high-speed serial bus architectures and these more advanced interface protocols.
 Most networking and communication systems sold and in operation today include line cards that process data at speeds ranging from 10Gbps to 400Gbps, and support many aggregated slower ports. To accommodate the substantial and growing increase in demand for networking communications and applications, networking systems manufacturers are developing and bringing to market next-generation systems that run at aggregate speeds of 400 Gbps or more with newer products scaling to tens of thousands of Gbps, or tens of terabits, per second. Applications, such as security appliances, broadcast video and compute accelerators that were previously running at aggregate rates of 10Gbps or 40Gbps, are moving to higher aggregate rates in the 100s of Gbps.   Although processor performance in applications, such as computing and networking has traditionally doubled nearly every 18 months, or even sooner, the performance of external high-density memory technology has generally been able to double only once every 10 years. Existing memory IC solutions built for high capacity and based on parallel interface architecture struggle to meet the access rates required to meet speeds of 100Gbps and beyond due to system-level limitations for pin counts, power and performance. To compensate for slow external memory access, developers must either integrate larger amounts of on-chip memory and/or utilize complex system alternatives to try to work around the access-rate limitations of these memories. The additional memory and circuitry adds to IC power, size and cost and may not be feasible depending on the economics and technology used to implement the data processor. These networking and communications systems generally comprise a chassis populated by 4 to 16 line cards. Often, these systems are shipped to customers with only a portion of the line card slots populated, and the customer will add additional line cards to increase system performance, capacity and features.
 Each line card requires a significant amount of memory to support its processing capabilities. Traditional external memory IC solutions currently used on line cards include both dynamic random access memory, or DRAM, and static random access memory, or SRAM. Line cards in networking systems use both specialized, high-performance DRAM ICs, such as reduced-latency DRAM, or RLDRAM, low-latency DRAM, or LLDRAM, and commodity DRAM, such as double data rate, or DDR ICs.  The latest DDR memory is high-bandwidth memory, or HBM, which provides high bandwidth, but has fundamentally slow access time.  For very high access, networking systems use higher-performance SRAM, which may be integrated into the data processing IC itself depending on size, power and economics or use a traditional external SRAM IC, such as quad data rate, or QDR SRAM. These memories are very fast, but are much smaller, cost more and burn more power than traditional DRAM. Substantially all of these traditional memory IC solutions use parallel interfaces, which are slower than serial interfaces. For data processing solutions, which are unable to integrate sufficient amounts of SRAM, such as FPGAs, we believe the external SRAMs or RLDRAMs will be increasingly challenged to meet the performance, pin count, area and power requirements as networking systems and other new security, video, and compute systems expand beyond 400Gbps. The result is a gap between processor and memory performance. To meet the higher performance requirements being demanded by the industry, while using current components and architectural approaches, system designers must add more discrete memory ICs to the line cards and/or add more embedded memory on the packet processor. New processor and custom data processing engine ICs are being developed that integrate more SRAM to help offset the bottlenecks, but the cost to develop these custom ICs is high and there is a trade-off in cost, power and size. 
 5
 
FPGAs offer flexibility, lower development cost and time to market but are limited in the amount of internal circuitry and the amount of integrated SRAM memory. We believe our Accelerator Engine family of products is well suited to address memory access bottleneck challenges and provide significant performance, size, pin count and power advantages compared to traditional external memory solutions, primarily for FPGA-based systems.
 In order to improve performance and resolve memory bottlenecks, in recent years, the trend has been to have algorithms on the memory device perform computations in order to reduce processing time and power consumption. This trend is sometimes called in-memory compute or processor-in-memory. In order to make a flexible solution, the in-memory compute can be accomplished with arrays of reduced instruction set computer, or RISC, cores on the memory device. Further performance gains can be accomplished with application-specific enhancements to the memory device’s instruction set architecture. 
 We have developed our ICs to synergistically address the need for high-speed data access and throughput currently confronting system designers. We expect our IC products to meet the increasing demands placed on conventional memory technology used on the line cards in high-speed systems. We believe that our products and technology are well positioned as replacements for existing IC solutions in order to support the needs of a growing number of FPGA-based data processing applications with aggregate rates greater than 100Gbps that require high bandwidth and high access rate to memory.
 Our Approach
 We have leveraged our proprietary IP to design our IC products and related acceleration IP to help OEMs in our target markets to address the growing bottlenecks in system performance. We have incorporated critical features into our product families to accomplish this objective.
 High-Performance Interface
 High-speed, efficient interfaces are critical building blocks to meet high data transfer rate requirements for communication between ICs on network line cards. Semiconductor companies are increasingly turning to serial interface architectures to achieve needed system performance. Using serial interfaces, IC developers also are able to reduce the number of pins (the wired electrical pins that connect an IC to the network line card on which it is mounted) on the IC. With reducing geometries, the size of most high-performance ICs is dictated by the number of pins required, rather than the amount of logic and memory embedded in the chip. As a result, using a serial interface facilitates cost reduction and reduced system power consumption, while improving the performance of both the IC itself and the overall system. While serial interfaces provide significantly enhanced performance over parallel interfaces, SerDes interfaces traditionally have had higher power consumption, which is a challenge for IC designers. Our SerDes interfaces, however, are optimized to meet our customers’ signal integrity, low-power consumption and latency requirements.
 We make our interface technologies compliant with industry standards so that they can interoperate with interfaces on existing ICs. In addition, we make them programmable to support multiple data rates, which allows for greater flexibility for the system designer, while lowering development and validation costs. 
 GigaChip Interface Protocol
 In addition to the physical characteristics of the serial interface, the protocol used to transmit data is also an important element that impacts speed and performance. To address this and complement our Accelerator Engine devices, we have developed the GigaChip Interface, or GCI, which is an open-interface transport protocol optimized for efficient chip-to-chip communications. The GCI electrical interface is compatible with the current industry standards, including 10G and 25G IEEE and OIF interface standards, to simplify electrical interoperability between devices. GCI can enable highly efficient serial chip-to-chip communications, and its transport efficiency averages 90% for the data transfers it handles. GCI is included in our ICs and is offered to customers and prospective partners on terms intended to encourage widespread adoption.
 6
 
High-Performance and High-Density Memory Architecture
 The high density of our proprietary 1T-SRAM technology stems from the use of a single-transistor, or 1T, which is similar to DRAM, with a storage cell for each bit of information. Embedded memory utilizing our 1T-SRAM technologies is typically two to three times denser than the six-transistor storage cells used by traditional SRAM. Embedded memory utilizing our 1T-SRAM technologies typically provides speeds essentially equal to or greater than the speeds of traditional SRAM and DRAM, particularly for larger memory sizes. Our 1T-SRAM memory designs can sustain random access cycle times of less than three nanoseconds, significantly faster than DRAM technology. Embedded memory utilizing our 1T-SRAM technologies can consume as little as one-half the active power and generate less heat than traditional SRAM when operating at the same speed. The 1T-SRAM allows us to integrate more high-performance memory using less expensive processing technology, reduce system level heat dissipation and enable reliable operation using lower-cost packaging. 
 Embedded In-Memory Functions
 We have combined our high-speed memory architecture with intelligence to define an embedded memory that can execute embedded functions and algorithms internally, or “in-memory,” to allow software and hardware designers acceleration options to improve the performance of their applications.
 The in-memory functions executed within the memory architecture in our Accelerator Engine IC products result in application-performance increases by reducing the number of external memory and computational operations needed to accomplish the same functions using traditional memories.  Also, by executing in-memory, the resources of the packet processor and other ICs on a customer board are available to perform other functions.
 Our Accelerator Engine ICs include an arithmetic logic unit, or ALU, which enables the performance of mathematical operations on data. Moving certain processing functions from the host data processor IC to the Accelerator Engine IC through the use of this embedded ALU, reduces the number of processing transactions and frees the host data processor IC to perform other important networking or micro-processing functions.
 Our Programmable HyperSpeed Engine IC takes this concept one step further by incorporating integrated RISC processors optimized for processing data structures and graphs.  Our Programmable HyperSpeed Engine IC integrates RISC cores optimized for operating data stored in the memory block.  The integration of the cores with memory allows system algorithms or functions to be offloaded to the device and reduces overall system-task latency and increases throughput.  The processors can be programmed by the user to offload and accelerate standard and/or customized functions from the main processor thereby reducing memory transactions and data path complexity to provide improved performance and lower system latency.  New algorithms or functions can be added to or modified in the Programmable HyperSpeed Engine IC in software.
 Our Strategy
 Our primary business objective is to be a profitable IP-rich fabless semiconductor company offering ICs and related software and IP that deliver unparalleled memory bandwidth and access rate performance for high-performance data processing in cloud networking, security appliances, video, test and monitoring, and data center systems. The key components of our strategic plan include the following strategies:
Target Large and Growing Markets
 Prior to 2019, our primary focus was the multi-billion dollar networking, telecommunications, security appliance and data center OEM equipment markets, as our products were developed to support the growth in 100Gbps and higher networking speeds. During 2019, we expanded our market focus to new markets, including video, test and measurement and computing markets. We are currently supporting customers across these markets, with whom we have achieved design wins. We define a design win as a commitment from a customer to utilize one of our IC products in its system. We continue to actively pursue additional design wins for the use of our ICs in our target markets. We believe our design wins represent the potential for future revenue growth. However, there is no assurance that these customer designs will be shipped in large volume by our customers to their customers, how much revenue each design win is likely to generate, or how much revenue all of these (and future design wins) are likely to generate.
7
 
Build Long-Term Relationships with FPGA Vendors and Suppliers of Data Processing Solutions
 We believe that having long-term relationships with FPGA providers is critical to our success, as such relationships enable us to reduce our time-to-market, provide us with a competitive advantage, identify additional IC design win and licensing opportunities and expand our target markets. A key consideration of network system designers is to demonstrate interoperability between our IC products and the processor ICs  utilized in their systems. To obtain design wins, we must demonstrate this interoperability, and also show that our IC products work optimally with the packet processor to achieve the performance requirements. In addition, our current strategy requires packet processor suppliers to adopt our GCI interface. To that end, we have been working closely with FPGA and application specific standard product providers to enable interoperability between our Accelerator Engine IC products and their high-performance products. To facilitate the acceptance of our Accelerator Engine ICs, we have made available development and characterization kits for system designers to evaluate and develop code for next-generation networking systems. Our characterization kits are fully-functional hardware platforms that allow FPGA and ASIC providers, and their customers, to demonstrate interoperability of the Accelerator Engine IC with the ASIC or FPGA the designers use within their systems. 
 Our IC Products 
 Accelerator Engines
 Our Accelerator Engine IC products, include the Bandwidth Engine, which is targeted for high-performance applications where throughput is critical, and the Programmable HyperSpeed Engine, which combines the features of the Bandwidth Engine with 32 RISC processors to allow user-defined functions or algorithms to be embedded in the Programmable HyperSpeed Engine.
Bandwidth Engine
 The Bandwidth Engine is a memory-dominated IC that has been designed to be a high-performance companion IC to packet processors. While the Bandwidth Engine primarily functions as a memory device with a high-performance and high-efficiency interface, it also can accelerate certain processing operations by serving as a co-processor element. Our Bandwidth Engine ICs combine: (1) our proprietary high-density, high-speed, low latency embedded memory, (2) our high-speed serial interface technology, or SerDes, (3) an open-standard interface protocol and (4) intelligent access technology. We believe an IC combining our 1T-SRAM memory and serial interface with logic and other intelligence functions provides a system-level solution and significantly improves overall system performance at lower cost, size and power consumption. Our Bandwidth Engine ICs can provide up to and over 6.5 billion memory accesses per second externally and 12 billion memory accesses per second internally, which we believe is more than three times the performance of current memory-based solutions. They also can enable system designers to significantly narrow the gap between processor and memory IC performance. Our customers that design Bandwidth Engine ICs onto the line cards in their systems will re-architect their systems at the line-card level and use our product to replace traditional memory solutions. When compared with existing commercially available solutions, our Bandwidth Engine ICs may:

• | provide up to four times the performance;
--+------------------------------------------


• | reduce power consumption by approximately 50%;
--+-----------------------------------------------


• | reduce cost by greater than 50%; and
--+-------------------------------------


• | result in a dramatic reduction in IC pin counts on the line card.
--+------------------------------------------------------------------

 Our Bandwidth Engine 2 IC products contain 576 megabits, or Mb, of memory and use a SerDes interface with up to 16 lanes operating at up to 12.5Gbps per lane. We have been shipping our Bandwidth Engine 2 IC products since 2013. We continue to win new designs for this device family, and expect these products to be our primary revenue source for the foreseeable future.
 Our Bandwidth Engine 3 IC products contain 1152Mb of memory and use a SerDes interface with up to 16 lanes operating at up to 25Gbps per lane. Our Bandwidth Engine 3 ICs target support for packet-processing applications with up to five billion memory single word accesses per second, as well as burst mode to enable full duplex buffering up to 400 Gbps for ingress, egress and oversubscription applications. The devices provide benefits of size, power, pin count and cost savings to our customers. 
 8
 
Programmable HyperSpeed Engine
 Our Programmable HyperSpeed Engine IC products further leverage our proven serial interface technology and high-density integrated memory with the processor engine architecture to enable high-speed customizable search, security, and data analysis functions for networking, security, and data center applications, as well as new markets such as video and compute acceleration. The product architecture features 32 search-optimized processor engines, data flow schedulers, and over a terabit of internal access bandwidth. The device leverages our GCI interface technology and high-density integrated memory (1152Mb of 1T-SRAM embedded memory). 
QPR
 During 2020, we launched a new line of memory ICs, our quad partition rate, or QPR, family of low cost, ultra-high speed SRAM memory devices optimized for FPGA-based systems.
 Our QPR memory technology features an architecture that allows for parallel accesses to multiple partitions of the memory simultaneously and allows access of up to 576 bits per read or write cycle. The QPR device includes four independent partitions per input/output and each partition functions as a stand-alone random-access SRAM. The high-performance interface, larger density and the multiple partitions work together to support multiple independent functional blocks within an FPGA with one QPR device. The MoSys MSQ220 and MSQ230 QPR devices are ideally suited for random-access applications. MoSys also offers an optional FPGA RTL memory controller to simplify the interface to its high capacity 567Mb or 1Gb devices. We also offer an RTL memory controller that presents an SRAM-like interface to simplify the QPR design effort.
 The target applications are FPGA-based and include a broad range of markets, including test and measurement, 5G networks, router, switching, security, computational storage, database acceleration, Big Data, aerospace and defense, advanced video, high-performance computing, machine learning and AI and other data-driven areas.
 
LineSpeed Flex PHYs
 Our LineSpeed Flex family of 100G PHYs, is designed to support industry standards and includes gearbox, Multi-Link Gearbox, or MLG, and high density CDR/retimer devices designed to enable Ethernet and OTN line card applications to support the latest electrical and optical interfaces.
 IP Licensing
 1T-SRAM
 Historically, we licensed our IT-SRAM memory and SerDes interface technologies on a worldwide basis to semiconductor companies, electronic product manufacturers, foundries, intellectual property companies and design companies.  Most of these licensees incorporated our technology into ICs that they sold to their customers, and, in the case of IT-SRAM licenses, pay a royalty to us for each IC shipped that incorporates our technology. Royalty and other revenue generated from our legacy IP agreements represented 12% and 7% of our total revenues for 2020 and 2019, respectively. 
 Virtual Accelerator Engines
Recently, we announced our new VAE product line that consists of software, firmware and other IP, such as register-transfer level, or RTL, code and utilizes a common application programming interface and common RTL interface to facilitate platform portability. This new product line will include multiple function accelerator platform products, which target specific application functions and will use a common software interface to allow performance scalability over multiple hardware environments. These function accelerator platform products are hardware agnostic and operate with or without one of our Accelerator Engine ICs. For example, our VAE IP can run on a processing unit IC or FPGA that is not attached to a MoSys IC or an FPGA that is attached to a MoSys IC, such as the Bandwidth Engine or Programmable HyperSpeed Engine.
 Our initial VAE product is our graph memory engine, or GME, accelerator IP, which is part of our packet classification platform, for performing embedded search and classification of packet headers. A typical use would be an alternative to ternary content-addressable memory, or TCAM, which is a specialized type of high-speed memory that searches its entire contents in a single clock cycle. While TCAMs enable the highest levels of performance, they are monolithic ICs that are limited in capacity and consume large amounts of power. In comparison, our GME IP can be integrated into the existing processor chip or chipset with no additional stand-alone IC required. Our 
 9
 
proprietary platform software enables the compilation of TCAM images into graphs for GME processing utilizing a wider range of memory types including DRAM. 
 We believe the technology will generate new opportunities that require less up-front architectural changes by system designers and provide a scalable capacity and performance roadmap of options using our Accelerator Engine ICs. We began pursuing license opportunities for our VAE products in 2020 and expect to begin achieving production licenses for these products in 2021.
 Research and Development
 Our ability to compete in the future depends on successfully improving our technology to meet the market’s increasing demand for higher performance and lower cost solutions. Development of new IC products requires specialized chip design and product engineers, as well as significant fabrication and testing costs, including mask costs.  We currently do not have internal resources for new IC products. That said, we believe our Accelerator Engine IC product portfolio will provide us with adequate revenue growth opportunity for the foreseeable future.  We have focused our product development efforts on software-based capabilities and features that leverage our current technologies and core competencies and complement, our Accelerator Engine IC products. As discussed above, we recently announced our VAE product line, and the initial packet classification products will use our graph memory engine for performing embedded search and classification of packet headers. We intend to continue to devote the majority of our research and development efforts toward furthering our VAE product roadmap and, where applicable, developing customer-specific IP. 
Sales and Marketing
 We believe that systems OEMs typically prefer to extend the use of traditional memory solutions and their parallel interfaces, despite performance and costs challenges, and are reluctant to change their technology platforms and adopt new designs and technologies, such as serial interfaces, which are an integral part of our product solutions. Therefore, our principal selling and marketing activities to date have been focused on persuading these OEMs and key component specialists that our IC products provide critical performance advantages, as well as on securing design wins with them.
 In addition to our direct sales personnel, we sell through sales representatives and distributors in the United States and Asia. During 2020, we entered into new distribution relationships with Arrow Electronics and DigiKey Electronics, which are two of the largest worldwide IC distributors. These distributors have a global presence with offices and technical selling and applications, which we believe will enable us to reach new potential customers for our products.
 We also have applications engineers who support our customer engagements and engage with the customers’ system architects and designers to propose and implement our IC and IP solutions to address system design challenges and improve performance.
 In the markets we serve, the time from a design win to production volume shipments of our IC products can range from 18 to 36 months. Networking, communications and security appliance systems can have a product life from a few years to over 10 years once a product like ours has been designed into the system. Our revenue has been highly concentrated, with a few customers accounting for a significant percentage of our total revenue.
  The following customers accounted for 10% or more of our net revenues in one or more of the following periods:
  
 | Year Ended 
-------------------+-------------
 | December 31,
 | 2020 | 2019
Flextronics | 37% | 30% 
Sanmina | 22% | 14% 
Clavis | 10% | 17% 
Nokia/ALU | 10% | * 
Palo Alto Networks | * | 13% 

 *Represents less than 10% 
 10
 
Intellectual Property
 We regard our patents, copyrights, trademarks, trade secrets and similar intellectual property as critical to our success and rely on a combination of patent, trademark, copyright, and trade secret laws to protect our proprietary rights.
 As of December 31, 2020, we held 66 U.S. and 33 foreign patents on various aspects of our technology, with expiration dates ranging from 2022 to 2037. We also held 4 pending patent applications in the U.S. and abroad. There can be no assurance that others will not independently develop or patent similar or competing technology or design around any patents that may be issued to us, or that we will be able to successfully enforce our patents against infringement by others.
 The semiconductor industry is characterized by frequent litigation regarding patent and other intellectual property rights. Our IC customers, licensees or we might, from time to time, receive notice of claims that we have infringed patents or other intellectual property rights owned by others. Our successful protection of our patents and other intellectual property rights and our ability to make, use, import, offer to sell, and sell products free from the intellectual property rights of others are subject to a number of factors, particularly those described in Part I, Item 1A, “Risk Factors.”
 Competition
 The markets for our products are highly competitive. We believe that the principal competitive factors are:
  
• | processing speed and performance;
--+----------------------------------


• | density and cost;
--+------------------


• | power consumption;
--+-------------------


• | reliability;
--+-------------


• | interface requirements;
--+------------------------


• | ease with which technology can be customized for and incorporated into customers’ products; and
--+------------------------------------------------------------------------------------------------


• | level of technical support provided.
--+-------------------------------------

 We believe that our products compete favorably with respect to each of these criteria. Our proprietary 1T-SRAM embedded memory and high-speed serial interface IP can provide our Accelerator Engine ICs with a competitive advantage over alternative devices. Alternative solutions are either DRAM or SRAM-based and can support either the memory size or speed requirements of high-performance networking systems, but generally not both. DRAM solutions provide a significant amount of memory at competitive cost, but DRAM solutions do not have the required fast access and cycle times to enable high-performance. The DRAM solutions currently used in networking systems include RLDRAM from Micron Technology, Inc., or Micron, LLDRAM from Renesas, DDR from Samsung Electronics Co., Ltd., Micron and others, and HBM, which is stacked DRAM memory from Samsung Electronics Co. and SK Hynix. SRAM solutions can meet high-speed performance requirements, but often lack adequate memory size. The SRAM solutions currently used in networking systems primarily include QDR or similar SRAM products from Cypress Semiconductor Corporation and GSI Technology, Inc. Most of the currently available SRAM and DRAM solutions use a parallel, rather than a serial interface. To offset these drawbacks, system designers generally must use more discrete memory ICs, resulting in higher power consumption and greater utilization of space on the line card.
 11
 
Our competitors include established semiconductor companies with significantly longer operating histories, greater name recognition and reputation, large customer bases, dedicated manufacturing facilities and greater financial, technical, sales and marketing resources. This may allow them to respond more quickly than us to new or emerging technologies or changes in customer requirements. Generally, customers prefer suppliers with greater financial resources than we have currently. Many of our competitors also have significant influence in the semiconductor industry. They may be able to introduce new technologies or devote greater resources to the development, marketing and sales of their products than we can. Furthermore, in the event of a manufacturing capacity shortage, these competitors may be able to manufacture products when we are unable to do so.
 Our Accelerator Engine 
ICs compete with embedded memory solutions, stand-alone memory ICs, including both DRAM and SRAM ICs, ASICs designed by customers in-house to meet their system requirements, and NPUs that use significant internal memory and customer-designed software to implement tasks. Our prospective customers may be unwilling to adopt and design-in our ICs due to the uncertainties and risks surrounding designing a new IC into their systems and relying on a supplier that has limited history of manufacturing such ICs and limited financial resources. In addition, our Accelerator Engine ICs require the customer and its other IC suppliers to implement our chip-to-chip communication protocol, the GCI interface. These parties may be unwilling to do this if they believe it could adversely impact their own future product developments or competitive advantages, or, if they believe it might complicate their development process or increase the cost of their products. To remain competitive, we believe we must provide unparalleled memory IC solutions with the highest bandwidth capability for our target markets, which solutions are engineered and built for high-reliability carrier and enterprise applications. 
 Our LineSpeed PHY ICs compete with solutions offered by Broadcom Ltd., Inphi Corporation, M/A-COM Technology Solutions Holdings, Inc. and Semtech Corp., as well as other smaller analog signal processing companies. We also may compete with ASICs designed by customers in-house to meet their system requirements, as well as by optical module OEMs. The market for our LineSpeed products is highly competitive, and customers have a number of suppliers they can choose from. We must provide differentiated features with a reasonable IC power budget, while offering competitive pricing.  To date, we have had limited success selling and marketing these products.
 Manufacturing
 We depend on third-party vendors to manufacture, package, assemble and test our IC products, as we do not own or operate a semiconductor fabrication, packaging or production testing facility. By outsourcing manufacturing, we can avoid the high cost associated with owning and operating our own facilities, allowing us to focus our efforts on the design and marketing of our products.
 We perform an ongoing review of our product manufacturing and testing processes. Our IC products are subjected to extensive testing to assess whether their performance meets design specifications. Our test vendors provide us with immediate test data and the ability to generate characterization reports that are made available to our customers. We have achieved ISO 9001:2015 certification, and all of our significant manufacturing vendors have also achieved ISO 9001 certification.
 Employees
 As of December 31, 2020, we had 24 employees all of whom are located in the United States, consisting of 15 in research and development and manufacturing operations and 9 in sales, marketing and general and administrative functions. 
Available Information
 We were founded in 1991 and reincorporated in Delaware in 2000. Our website address is www.mosys.com. The information in our website is not incorporated by reference into this report. Through a link on the Investor section of our website, we make available our annual reports on Form 10-K, quarterly reports on Form 10-Q, current reports on Form 8-K, and any amendments to those reports filed or furnished pursuant to Section 13(a) or 15(d) of the Exchange Act, as soon as reasonably practicable after they are filed with, or furnished to, the Securities and Exchange Commission, or SEC. You can also read any materials submitted electronically by us to
 the SEC on its website (www.sec.gov), which contains reports, proxy and information statements, and other information regarding issuers that file electronically with the SEC, including us.

 12
 
