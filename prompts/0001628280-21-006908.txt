Prompt: Summarize the business model from the following text. Answer with a continuous text and with fivehundredtwelve tokens at max. Set your focus on sources of revenue , the intended customer base , products , distribution channels  and details of financing. Use only information from the following the text:

ITEM 1. BUSINESS.
Overview 
Our vision is to make autonomous transportation safe and ubiquitous. As a global leader in lidar autonomous driving technology, we are enabling the world’s first autonomous solutions for automotive series production in passenger cars and commercial trucks. 
Founded in 2012 by President and Chief Executive Officer Austin Russell, Luminar built a new type of lidar from the chip-level up, with technological breakthroughs across all core components. As a result, we have created what we believe is the only lidar sensor that meets the demanding performance, safety, and cost requirements for Level 3 through Level 5 autonomous vehicles in production, bypassing the traditional limitations of legacy lidar technology, while also enabling Level 0 through Level 2 (Advanced Driving Assistance Systems (“ADAS”) and/or Luminar Proactive Safety) with our Proactive Safety solution. Integrating this advanced hardware with our custom developed software stack enables a turn-key autonomous solution to accelerate widespread adoption across automakers at series production scale. 
Our lidar hardware and software products help set the standard for safety in the industry, and are designed to enable accurate and reliable detections of some of the most challenging “edge cases” autonomous vehicles can encounter on a regular basis. This is achieved by advancing existing lidar range and resolution to new levels, ensuring hard-to-see objects like a tire on the road ahead or a child that runs into the street are not missed, as well as by developing our software to interpret the data needed to inform autonomous and assisted driving decisions. 
Our full-stack hardware and software autonomy solution for cars and trucks as well as our standalone lidar technology offerings have made us one of the leading partners for the world’s top OEMs. We are currently partnering with eight of the top-ten global automakers, by sales, and have the goal of being the first lidar company to produce highway self-driving and next-generation Proactive Safety systems for series production. With approximately 400 employees across eight global locations, we have scaled to over 50 partners in the last two years, including the first industry-wide automotive series production award in the autonomous space, awarded by Volvo Cars in May 2020, with series production expected to commence in 2022. We subsequently entered into a strategic partnership with Daimler Truck AG in October 2020 and with Mobileye Vision Technologies Ltd (“Mobileye”) in November 2020. 
The Luminar Difference 
We have established ourselves as a global leader in lidar autonomous driving technology, and these are the strengths that not only set us apart today, but we believe will continue to differentiate us in the future: 
Breakthrough Technology Delivering What We Believe is the World’s First Auto-Grade Compliant Solution.  Reflecting roughly nine years of development at this stage (the first five of which were in stealth), Luminar offers a unique lidar architecture and proprietary component-level innovation (built from the chip-level up), resulting in superior range and resolution capabilities, ensuring confidence in perception across a broad set of operational domains and unlocking the next 
generation of vehicle safety. Our lidar and perception software are built upon a longer wavelength lidar design, which has been widely embraced as necessary to broadly deploy truly autonomous vehicles. As a result, we believe that we are the only provider of lidar for automotive autonomy applications that achieves the industry’s stringent requirements and perception capabilities. Our technological prowess and differentiated approach is supported by an extensive intellectual property portfolio of 93 issued patents, in addition to 84 pending or allowed patents as of February 2021. 
Highway Autonomy.  By developing and deploying the industry’s first lidar technology to meet the stringent requirements required to enable highway autonomy, Luminar will provide its key customers with a dramatic step-function in performance and help enable the first wave of autonomous vehicles—hands-off, eyes-off autonomy for highway-related use cases—which we envision to be rolled out beginning late next year. 
Proactive Safety.  In addition to enabling hands-off, eyes-off autonomy for highway-related use cases, we see a significant opportunity for our lidar sensing system and software to enhance current ADAS functionality and safety; and reduce collisions across a variety of other operating domains in a proactive rather than reactive capacity. As a result, we foresee insurance-related opportunities, which may either accelerate the adoption of our integrated solution and help to cross-subsidize the implied cost of our system, aided in part through improved economies of scale. 
Deeply Integrated Hardware/Software Solution. We believe our Sentinel software offering provides our customers with a turnkey solution that accelerates the ability for OEMs to deliver high-speed highway autonomy and Proactive Safety at commercial series production scale. With over-the-air software updates, the product will be continually refined to ensure continued solution reliance and enhanced performance. 
Volvo Series Production Contract. In May 2020, we announced a landmark deal with Volvo Cars for the first automotive series production award for autonomy in the industry. As a result, our hardware and software could power Volvo’s next-generation vehicle platform, called SPA2, on which its future consumer vehicle models will be based. The intent of the program is primarily to enable highway autonomous drive capability as an option on production consumer vehicles, with series production expected to start in 2022. Additionally, the program presents an opportunity to simultaneously enable next-generation Proactive Safety systems in a more widespread capacity at lower cost than autonomous drive upgrades. 
Additional Commercial Success with Daimler Truck AG and Mobileye. In October 2020, we announced a strategic partnership with Daimler Truck AG, the world’s largest commercial vehicle manufacturer, to enable highly automated trucking, starting on highways. Our teams work closely together in order to enhance lidar sensing, perception, and system-level performance for Daimler trucks moving at highway speeds. To strengthen the partnership, Daimler Trucks has acquired a minority stake in Luminar. In November 2020, we executed a contract with Mobileye, an Intel company, to supply Luminar lidar for use in Mobileye’s first generation of its Level 4 Mobility-as-a-Service (MaaS) pilot and driverless fleet in key markets around the world. 
Compelling Growth, Margin, and Cash Flow Profile. We believe that our robust customer base and growing list of commercial partnerships creates a compelling growth profile. This is further enhanced by the visibility to series production from existing and developing agreements that would enable rapid growth. Our product cost structure includes exclusive supply agreements for all three of our key lidar components (receiver, ASIC, and laser), enabling us to achieve significant material cost reductions as volume increases for such key hardware components. As we scale production and grow our revenue, we believe our strategy of low capital intensity provides the potential for high shareholder return. 
Deep Bench of Industry Leaders. We have a visionary leadership team with a track record of innovation and execution, led by our President and Chief Executive Officer, Austin Russell, to develop a new kind of sensing technology to make autonomous vehicles both safe and ubiquitous. With approximately 400 employees across 8 global locations (including a millennia of man and woman years of lidar-related experience), Luminar has built a deeply experienced team of industry leaders from across the lidar, automotive, technology and autonomy sectors, including senior members from automotive companies such as Daimler-Benz, ZF, VW and Harman and technology companies such as Google, Uber, Motorola and Ocean Optics. 
Our Market Position and Leadership
We were founded with the vision of making autonomous transportation safe and ubiquitous. As a global leader in lidar autonomous driving technology, we are enabling the world’s first autonomous solutions for automotive series production in passenger cars and commercial trucks. 
The automotive industry is among the largest in the world and features an estimated total addressable market opportunity (“TAM”) for ADAS and autonomous solutions (Level 0 through Level 5) expected to exceed $150 billion by 2030. Our model to capture this opportunity is to directly partner with top established automotive companies in order to power their autonomous future. Correspondingly, we have successfully established partnerships with over 50 companies across three primary application verticals: passenger vehicles, commercial trucks, and robo-taxis. More than 75% of the companies listed in the target ecosystem 
chart below are working with Luminar customers. Although not our primary focus, adjacent markets such as aerospace, defense and smart cities offer use cases uniquely suited for and potentially served by our technology.
An important benefit of our engagements with commercial partners is to have our products generally incorporated into our commercial partners’ development programs at the earliest stages. By securing these development wins in a competitive landscape, there is greater increased forward visibility into the long-term development cycle towards series production. This awards us with a significant competitive advantage by positioning us to convert existing development engagements with key automakers into series production awards in the near term, as we have with Volvo Cars and others we expect to finalize in the future. 
We have a number of OEM, trucking and robo-taxi-related partners currently in the process of validating our technology, principally using our Hydra lidar sensors (described further below), which is geared toward research and development fleets. We also have a significant number of advanced development partners, in which we see an opportunity to convert into series production awards through 2022. We expect that all series production partners will use our Iris lidar sensors (described further below) for upwards of one million or more vehicles, building on the work already completed with Hydra. 
A majority of autonomous vehicle companies have been primarily focused on robo-taxi research and development for urban low speed ridesharing applications (of which we work with many). We are, by comparison, focused for the time being on the highway autonomy use case for production vehicles and are powering the substantial majority of autonomous trucking programs. This presents a unique opportunity for us to enable near-term production deployments over the next few years, while it is expected that higher levels of autonomy for urban robo-taxi applications will take substantially longer to reach scale. 
Driving further volume beyond highway autonomy is our Proactive Safety solution, with the goal of ultimately preventing the majority of forward collisions that occur on roads today. With over one million fatalities globally each year from vehicle accidents, there is a clear opportunity to set a new baseline standard for vehicle safety industry-wide. 
Current industry ADAS capabilities are enabled primarily by camera and/or radar sensing technologies. Data from both sensor types are commonly merged to provide the vehicle system with some understanding of its driving environment. These systems, however, fall short of delivering substantial safety gains. Today’s ADAS works well under ideal circumstances—at low speed, in ideal weather conditions, and on a test track—however with our Proactive Safety solution, we believe we can decrease the reported collisions occurrence rates by up to seven times. 
Launching this bold vision forward, we entered into a landmark deal with Volvo Cars for the first automotive series production award for autonomy in the industry, which was announced in May 2020. Our hardware and software is being integrated into Volvo’s global consumer vehicle platform to power autonomous highway driving and Proactive Safety features, with series production scheduled to take place in 2022. Volvo has historically been a leader in deploying new breakthrough safety-centric technologies into the automotive industry, ranging from the invention of the modern three-point seat belt to the launch of Mobileye’s vision-based ADAS product. 
In October 2020, we also announced a strategic partnership with Daimler Truck AG, the world’s largest commercial vehicle manufacturer (through its Freightliner and Western Star Brands), to enable highly automated trucking, starting on highways. Experts at Daimler Truck AG, along with its U.S. subsidiary, Daimler Trucks North America (DTNA) and Torc Robotics, part of Daimler Trucks’ Autonomous Technology Group, are collaboratively pursuing with Luminar a common goal of bringing series-produced highly automated trucks (Level 4) to roads globally. Our teams work closely together in order to enhance lidar sensing, perception, and system-level performance for Daimler trucks moving at highway speeds. To strengthen the partnership, Daimler Trucks has acquired a minority stake in Luminar. 
In November 2020, we executed a contract with Mobileye, an Intel company, to supply Luminar lidar for the company’s Autonomous Vehicle Series solution in its next phase of driverless car development and testing, in production volumes at sub-$1,000 cost. As part of the agreement, Mobileye will collaborate with us to use our lidar for the first generation of its Level 4 MaaS pilot and driverless fleet in key markets around the world. Our technology will be used to enable Mobileye’s TRUE REDUNDANCY capability, with multiple self-contained sensor systems to enable uncompromised safety and validation for Level 4 driving.
In March 2021, we announced a partnership with Zenseact to deliver autonomous software for series production vehicles. Volvo Cars is the first launch customer, representing both Luminar’s and Zenseact’s first production design win for software. Luminar’s new product suite, Sentinel, is the first full-stack autonomous solution for series production in the industry. It deeply integrates Zenseact’s OnePilot autonomous driving software solution alongside Luminar’s Iris lidar, perception software, and other components as a foundation, enabling every automaker to offer Highway Autonomy and Proactive SafetyTM capabilities on their production vehicles. While the wider autonomous industry largely focuses on robo-taxi applications, Luminar and Zenseact collectively remain focused on delivering systems into series production vehicles.
In March 2021, we entered into a relationship with SAIC Motor Corporation, the largest automaker in China, pursuant to which Luminar is expected to power the autonomous capabilities and advanced safety features in SAIC’s new R brand vehicles for series production with its industry-leading lidar as well as components of its Sentinel software system. The R brand program is expected to begin series production with Luminar starting in 2022, with the parties’ longer-term goal being widespread standardization across all vehicle lines. As part of the close collaboration, we will also be establishing an office in China to be located in Shanghai alongside SAIC Motor, where SAIC would also be providing local support. The parties expect to deliver the first autonomous production vehicles in China, establishing SAIC’s technology leadership position and Luminar’s production launch in the region. 
Market Outlook 
There is a worldwide trend towards mobility and e-mobility and a renewed focus on autonomy, specifically highway autonomy for passenger and commercial vehicles. As the market shifts toward electric and hydrogen drivetrains, along with software-defined vehicles delivering a new user experience and data capability, we see the potential of autonomy enabled by the sensing and computing technologies on vehicles and under advanced development today. The roadmap from existing driver assistance and comfort features all the way to self-driving value can be built through improved vehicle situational awareness. 
Our products provide this situational awareness in a broad range of driving environments and allow for confident detection and planning at all vehicle speeds. Our portfolio encompasses sensor hardware, and perception and decision-making software that improve existing vehicle features and enable new levels of vehicle automation for consumer and commercial applications. To understand the ADAS and autonomy markets addressed by our products, it is important to understand the levels of automation as defined by The Society of Automotive Engineers (“SAE”).
Although SAE has clearly defined these levels, there continues to be inaccuracies and misuse of the levels leading to consumer misconceptions about the true capability of the vehicle which they purchase. We believe our lidar greatly enhances the lowest levels of autonomy and enables the deployment of the highest levels of autonomy to both the consumer and commercial markets. Below is a more detailed description of the levels of automation. 
•Level 0—Active Safety: In this level, the human is fully responsible for all driving functions at all times. “L0” is defined as driver support features that are limited to warnings or momentary driving assistance. Examples of warnings include blind spot warning or lane departure warnings. Examples of features with momentary assistance include automated emergency braking (“AEB”) and lane keep assist (“LKA”). These features are viewed as the basis of active safety, with AEB designed to reduce and/or mitigate the severity of low speed accidents, and LKA designed to prevent vehicles from crossing over into neighboring lanes or even worse, oncoming traffic. These features apply to both passenger and commercial vehicles and are growing as standard features globally and represent the majority of the ADAS market today. 
•Luminar value-add: Our lidar’s long range and high resolution capability enables the detection and classification of objects (vehicles, pedestrians, cyclists) in all lighting conditions and inclement weather. We expect this to greatly improve upon today’s systems, and to be much more effective at taking proactive measures to avoid accidents and extending the AEB capability to higher speed driving scenarios. Additionally, the ability to detect lanes out to 150 meters and do so in these same adverse environmental conditions adds to the robustness of LKA systems and helps prevent temporary loss of lanes or lack of detection altogether as often seen in today’s systems. 
•Levels 1 and 2—Driver Assist: These levels represent the last levels in which the driver is still fully responsible for all driving functions at all times. “L1” is defined as driver support features that provide steering or braking/acceleration assistance, but not both simultaneously. Examples include lane centering support (“LCS”) or the more widely adopted adaptive cruise control (“ACC”). These features are viewed as comfort features, easing the driving load from the driver during extended highway drives. “L2” captures multiple driving tasks, for example both ACC and LCS simultaneously. In the near future, we expect an increased adoption of these systems as safety protocols begin to require head-on collision assistance which will require simultaneous braking and steering control.
The term L2+ is often used for today’s higher capability systems, many of which add a driver monitoring camera to ensure the human driver remains engaged, but allow them to remove their hands from the wheel completely (eyes must remain on the road). These systems are currently restricted in Europe, but allowed in the United States and other regions of the world in the restricted operational design domain (“ODD”) of divided expressways, high-ways, and typically only in systems with onboard high-definition maps of those expressways. The ramp up of these systems has been slower on the market, mainly due to the additional sensing and compute costs for marginal value-add to the end consumer. 
• Luminar value-add: Similar to L0, we expect to greatly improve upon today’s L1 and L2 in performance, robustness and availability. With the ability to detect lanes and precisely measure the distance to a lead vehicle in a single lidar sensor, we can independently give lane assignments to objects ahead. This helps prevent false braking events while driving in ACC mode, making the consumer experience safer and more enjoyable. Add this to the ability to detect lanes independent of lighting conditions, and we add confidence and robustness to nighttime driver support systems as well. As driver confidence in these features grows, we expect the utilization and adoption of such features to increase, leading to higher impact of vehicle safety systems. 
•Levels 3 and 4—Highway Autonomy: In these levels, the vehicle can still be operated in normal driving mode. However, when the automated driving function is engaged, the human is no longer responsible for the driving function. “L3” requires that the human driver must take back complete control of the vehicle when requested. “L4” assures the vehicle will continue to function without any human driver intervention, even if in a degraded state. Terms such as “chauffeur” are used for L3, while terms like “pilot” are used for L4, sometimes incorrectly. Further, robo-taxis today are aspiring to L4 but still rely on safety drivers behind the wheel making them L3 systems – including leaders like Waymo. To better quantify a vehicle’s autonomous capabilities, the market has started to assign an ODD and while many are trying to enable L4 for the urban environment, the most logical ODD for L3 and L4 driving is divided expressway or highway. 
Subsequently, a vehicle may not have L4 capability from the garage or the docking facility to the highway, but from highway entrance to highway exit, the vehicle can provide L4 functionality for that specific ODD. In 2020, the L3 and L4 markets only exist in development platforms and there are no serial production automotive L3 or L4 systems available. We believe, however, this segment represents significant growth potential and when correctly implemented, will prove valuable to both the consumer and society. 
• Luminar value-add: Adding our lidar to these systems improves their robustness and availability, allows sensing redundancy to cameras and radar, and therefore enables true hands-off and eyes-off operation. This allows the driver to utilize their time for something other than supervising the driving function, which is the ultimate product purpose of autonomy. 
• Levels 4 and 5—Urban/Full Autonomy: “L5” is essentially the same as L4, but without the ODD restriction. It is the designation for vehicles that when placed in automated driving mode, can drive everywhere and in all conditions without human intervention or even occupants. We group this L4/L5 functionality due to the current focus on urban and suburban driving in the form of robo-taxis. Commercial trucking also aspires to L5 capability but is focusing its L4 efforts on highways as this yields the highest benefit. An urban L4 is extremely complicated compared with highway L4. We do expect that robo-taxis and automated people movers will be a strong growth market, but the timeline is more uncertain and we expect this market growth to be limited while technology for both vehicles and infrastructure matures. 
• Luminar value-add: Similar to L3 and L4, we believe lidar is required to deliver L5 sensing and perception needs. Sensing redundancy and multiple modalities are required and perhaps more important since the environment is the most complex, and our lidar’s sensing and perception capability supports the needs of detection and classification in dense, congested, and difficult environments at all hours of the day and night. 
While these SAE levels are important to technology developers, we believe the market is currently segmented in two distinct categories: (1) ADAS or driver-assistance, where a human is in the driving loop and responsible, at minimum, to be a safety fallback and in most circumstances directly control part or all of driving tasks; and (2) autonomous driving, where a human is “out-of-the-loop” (colloquially, “hands off” the steering wheel and “eyes off” the road), which generates real value propositions to consumers, such as allowing the driver to recover time, as opposed to mere comfort or novelty features. 
Within these two segments, we believe the largest business opportunities exist in the areas of active safety and highway autonomy due to trends in safety technology standardization and consumer pain-point priority. These two applications have well aligned technology requirements that allow us to remain focused on a single product/solution that will allow OEM partners to achieve both. The broader autonomy market segment, specifically robo-taxis, represents strong long-term opportunity, but lidar technology must be seeded now during development even though high-volume production and deployment remains many years away. 
These trends and safety needs apply to both the passenger and commercial vehicle markets. The autonomy use case and business case for commercial vehicles are simple: reduce operational costs and increase efficiencies. Passenger vehicles are more complex since the ability to deliver autonomy is more focused on the consumer’s comfort and convenience. We are working to help OEMs and consumers achieve these goals, but with the proper level of safety included. Our lidar is also making traction in other markets, including defense and smart cities, that require high resolution and long-range sensing in uncontrolled operating conditions. 
Source: Our estimates, incorporating data from IHS Markit and Wall Street research. Includes passenger and commercial vehicles (including robo-taxi) as well as hardware and software.
The charts above represent today’s market in 2020 for which scanning lidar is limited. The market, however, is expected to grow substantially by 2030 and our technology has the potential to improve or enable capability across the full spectrum of the market. Our initial focus for lidar technology is L3/L4, and we aim to offer the sensing, perception, and function turn-key system that will truly add value and give driving time back to the end consumer. This market is still developing, but represents significant growth, and we are the technology leader with the first L4 highway production platform win with Volvo. In addition, vehicles enabled with our lidar will be capable of proactive safety in which accidents are potentially completely avoided, which can benefit other autonomy solutions such as L1/L2. 
Passenger Vehicles 
The passenger vehicle market is very large. We expect that more than approximately 100 million new passenger vehicles will continue to be manufactured year-over-year through 2030 and beyond. It is very difficult to replicate this volume in other markets, but it is also important to recognize that highway autonomy is not yet standard equipment. In order to realize a vehicle feature’s maximum societal benefits, the ultimate goal in the automotive industry is to achieve widespread adoption of the highway autonomous feature in all vehicles. We expect a technology adoption ramp-up over time as automated functionality 
matures, costs and pricing are reduced, and consumers become more familiar with the full benefits and capabilities of a safe autonomy system. We believe there is a substantial market opportunity for our products when proactive safety is coupled with autonomy due to the public benefit of the overall anticipated safety increase. 
ADAS 
ADAS volumes are primarily driven by both the European and North American markets. The European New Car Assessment Program (“NCAP”) requires a minimum level of crash mitigation functionality such as AEB (for vehicles, pedestrians, and cyclists), LKA, speed alert systems and other ADAS features for a vehicle to have a 5-star rating. Furthermore, the European Union is moving toward mandates of these advanced functions. 
The U.S. is less focused on mandates at this time and instead allows the U.S. NCAP (known as the “Stars on Cars” program) and designations such as the Insurance Institute for Highway Safety “Top Safety Pick” and “Top Safety Pick+” to drive adoption and provide consumers with an understanding of the vehicle’s advanced crash avoidance capability. Additionally, in working with the National Highway Traffic Safety Administration (“NHTSA”), 20 automakers pledged to voluntarily equip virtually all new passenger vehicles by September 1, 2022 with a low-speed AEB system that includes forward-collision warning. With global safety rating programs and the OEMs competing to deliver more safety and comfort features to their customers, it is reasonable to expect near complete adoption of ADAS functionalities in new vehicles produced by Europe, U.S., Japan, and South Korea by 2026. We expect adoption rates to increase significantly in China as well. 
Tesla’s “Autopilot” is an example of establishing a driver support (as defined by SAE) platform as standard equipment. They developed a vehicle around the promise of future functionality which supports the production volume and cost reduction needed to spread technology beyond premium, low volume platforms. We expect more OEMs to demand proactive safety and limited autonomy with the ability to upgrade functionality over time without hardware change. This expectation aligns well with the increasing number of OEMs developing new vehicle platforms that span their lineups. 
Proactive Safety 
While the increased application of existing ADAS technology should help lessen the number of accidents and fatalities, we believe there is significant room for improvement concerning standard ADAS and crash avoidance. Today, the ADAS systems are designed to mitigate or lessen the severity of accidents and only avoid them under certain low-speed or ideal environmental conditions. Recent data suggests that the number of automotive fatalities globally still exceeds one million annually and the social costs of accidents continue to exceed $500 billion in the United States alone. As the autonomy market matures, we expect that OEMs and global NCAP programs will extend the functionality to intersection and crossing scenarios, which requires wider fields-of-view and faster detection. Global safety rating programs are also considering night and low-light performance in the future, further pushing the existing technology’s limits. We believe there is a significant opportunity to be able to reduce collisions with a capable lidar sensing system and software which can enable an understanding of the environment, which can help to avoid collisions by taking over the steering wheel and braking systems proactively. Our lidar is capable of significantly increasing the effectiveness of these active safety systems and supports proactive safety and greater crash avoidance measures using our long-range, high resolution, wide Field-of-view, and perception software to be able to detect pedestrians and cyclists in the most challenging and complicated environmental sensing conditions. Furthermore, high-speed safety performance, specifically AEB, is increasingly important as hands-free highway driving assist systems are further delivered to the market, and the vehicles take on more of the driving responsibility. 
Highway Autonomy 
Since inception, our focus has been to enable safe and ubiquitous autonomy and we view highway autonomy, in combination with proactive safety, as providing the most value to the end consumer for the foreseeable future. The market is also trending in this direction, targeting hands-off and eyes-off operations in a more controlled setting than the urban environment. While there is a significant focus on investment and development of robo-taxi solutions, passenger vehicles continue to be a voluminous market, and we expect the growth rate of highway automated functions to have a compound annual growth rate (CAGR) of nearly 40% from 2020 until 2030. 
Commercial Trucking Market Outlook 
The amount of goods transported by trucking globally continues to rise year-over-year. While the number of newly manufactured trucks has declined in recent years, the application of ADAS technology continues to grow and the interest in autonomy for transport is at an all-time high. The business case for trucking highway autonomy is simple: lower operating costs and increased availability of the vehicles and time spent on the road (trucking and fleet companies do not get paid to park at rest stops). 
The application of AEB has been in the market for many years, with the first mandate for vehicle AEB in Europe in 2013, and growing application of the functionality since. Similar to passenger vehicles, Europe leads the market in a unified safety direction and has put mandates in place to drive lane keeping functions and expand the AEB functionality to include 
pedestrians and cyclists. This leadership is also a result of a market driven by the trucking manufacturers who set the technology distribution of vehicles and the ADAS vehicles and systems architectures. Unfortunately, the trucking market in North America is heavily driven by the fleet operators’ specifications and is heavily fragmented. The lack of mandates from governing bodies has resulted in a market for ADAS that is very difficult to quantify and gain economies of scale across a small set of partners as is the case in Europe. As in passenger vehicles, our lidar technology and sensing capability could greatly improve the L0 and L1 functionality for the trucking market as well. However, our focus and the value add seen globally by the OEMs and fleet operators is L4 highway autonomous driving. 
L4 highway autonomy is the target ODD for trucking because that is where their money is earned and where the majority of the physical truck’s time is spent. The sensing needs between Europe, North America, South Korea, Japan, and other regions globally all differ slightly, but have similarities in the requirement for (i) long range detection to aid in extra braking time, (ii) farther detection of lanes to aid in proper lane centering and placement of potential obstacles in the correct lanes, and (iii) the vertical field of view and high placement on the cab to support close proximity detection in front of the vehicle, as well as overhead obstacles (such as bridges and overhead signs). 
Robo-Taxi and Delivery Market Outlook 
The press announcements of large robo-taxi investment and partnerships between technology companies, both established and startup, and mainstays from the automotive industry dominate the industry’s attention. This application is, however, the most difficult vehicle autonomy feature to solve for technically. It requires the ability to detect and classify hundreds of objects and predict motion for many of those objects, including pedestrians, electric scooters, and bicycles—all of which present as pedestrians, but move in very different ways. The environment consists of dynamic weather, steam from manholes and exhaust pipes, and oftentimes construction equipment causing dust and debris. Given the economic benefit an automated robo-taxi driving system could unlock, billions of dollars in funding and engineering efforts have been focused on developing solutions. The majority of the autonomous vehicle companies are operating in this space, awaiting a market that requires complex governmental support, funding for infrastructure, and a sensing and compute solution that must anticipate every possible mixed-traffic scenario. 
Additionally, the initial ODD only requires low to medium speed operation, which can be met with less capable sensors. We expect that ultimately, the ODD will need to expand to the highway as robo-taxis and automated shuttle services move people from city centers to the airport and back, in particular. We expect limited robo-taxi R&D programs will continue to operate in varying levels of development and testing the rest of this decade. 
Adjacent Markets 
Although not our primary focus, the adjacent markets below offer use cases uniquely suited for and potentially served by our technology. Our goal is to scale our core markets and utilize our robust solutions to best serve these adjacent markets where it makes sense for us and our partners. 
•Smart Cities. Many government agencies are motivated to invest in smart cities solutions such as “Smart” intersections and “Intelligent” tolling systems due to macroeconomic trends such as usage of electric vehicles (and the subsequent reduction in fuel taxes) and growing city populations (and the subsequent need to manage assets more efficiently). As discussed above with trends of urban living and the need to manage traffic flow and congestion, not only is there a market for the vehicles themselves but also for the infrastructure to support such automation. Today, many global cities have a defined Smart City initiative to be delivered over the coming years, with over 50% of these initiatives being in Europe and North America. The market is broken up into segments: smart buildings, transportation, infrastructure, healthcare, energy, security, and education. We will focus on infrastructure and security: traffic flow and intersection management, tolling and traffic management, smart parking and security, pedestrian and crowd flow management and security, and large venue security. 
•Aerospace and Defense. The aerospace and defense markets are intent on increasing their autonomous capability and lidar is a key component to enabling such automation, including for items such as an automated convoy for resupply or an automated refueling mission. These markets represent a small volume, but with very specific requirements that only certain technologies will be able to meet. We will utilize our sensing and system architecture from our core automotive system and provide solutions in this space and/or partner with companies who can help deliver specific solutions licensing our high performance technology. 
Our Solution Overview 
We bring opportunity and inspiration to an automotive industry that requires continuous technological and performance innovation, and play a critical role in making the future of mobility safer. The hope for autonomy is not just novelty – it is the critical feature required to transform the way people and goods move throughout the world transportation ecosystem. Autonomy presents an opportunity to save lives through enhanced safety, liberate those who struggle with transportation access, 
and reoptimize value chains of logistics and vehicle ownership. We seize this opportunity by delivering what we believe is the world’s first autonomous solution for series production, powering highway autonomy and proactive safety. 
High-performance lidar is not just another sensor. While it is true that lidar is a sensor, its value is more than just hardware and delivering a point cloud “image.” It is similar to radar and cameras in that these devices provide no direct value without the signal processing, detection, tracking, and perception software that gives an understanding of the vehicle’s surroundings. The next product offering levels are to provide route planning and command the steering, braking, and engine actuators to control the vehicle. This will require lidar producers to follow the precedents set by camera and radar, where sensor providers supply perception software (they are, after all, the experts in that sensor’s data). 
Many companies have developed lidar sensors, but not all have developed lidar systems. A lidar product offering can be broken down as follows: 
Lidar: For customers with a full complement of vehicle system software development, this product enables their development of vehicle functions through a sensor hardware product. 
Highway Autonomy: A full vehicle function product combining hardware and software for driver out-of-the-loop on highways. 
Proactive Safety: A full vehicle function product combining hardware and software that continuously monitors, but only momentarily acts to avoid collisions on all road types. 
As the requirements of a lidar system increase, the number of competitors tends to quickly decrease. We were founded with the understanding that the most effective lidar solution will have perception that can deliver the complete desired solution through the OEM to the end consumer. Many OEMs, via their camera experience, have outsourced everything to the supply base, except function development. Many have outsourced even this functionality and are starting to weigh the benefit of having a proprietary solution to using a more standardized, off-the-shelf product that saves them time and money. 
Commercial Overview 
We partner with the majority of key OEMs focused across three verticals: passenger vehicles, trucking, and robo-taxi. More than 75% of the companies listed in the target ecosystem chart below are Luminar customers. Furthermore, we have strong demand for our products in multiple adjacent market verticals. 
An important benefit of our engagements with commercial partners is to have our products generally incorporated into our commercial partners’ development programs at the earliest stages. By securing these development wins in a competitive landscape, there is greater increased forward visibility into the long-term development cycle towards series production. This awards us with a significant competitive advantage by positioning us to convert existing development engagements with key automakers into series production awards in the near term, as we have with Volvo Cars and others we expect to finalize in the future. 
We have a number of OEM, trucking and robo-taxi-related partners currently in the process of validating our technology, principally using our Hydra lidar sensors (described further below), which is geared toward research and development fleets. We also have a significant number of advanced development partners, in which we see an opportunity to convert into series production awards through 2022. We expect that all series production partners will use our Iris lidar sensors (described further below) for upwards of one million or more vehicles, building on the work already completed with Hydra. 
In the near term, we are focused on the passenger vehicle and trucking markets, which we believe will drive our ability to increase market share and achieve economies of scale. 
Passenger Vehicles 
Due to the complexity and challenging environment of urban driving, we believe that the industry will focus on highway autonomy in the near future. Our series production award with Volvo, a global leader in automotive safety, is a key example. Our lidar technology will power Volvo’s first fully self-driving technology for highways in their next-generation production passenger vehicles, enabling true driver out-of-the-loop functionality, which we expect will set new standards of safety for the industry. 
By 2030, we anticipate we will have approximately 4% vehicle penetration rate across the industry. Today, a majority of our current partners have a highway autonomy program in development with an anticipated start of production year ranging from 2023 to 2025. Leveraging our hardware and software for series production also paves the way for future proactive safety use cases in vehicles. We believe our lidar unlocks greater crash avoidance capability than today’s active safety systems and will help deliver what it calls “proactive safety” to the consumer – higher speed emergency braking, enhanced lane keeping functionality, and significantly improved performance and availability in inclement weather and low-visibility conditions. Given our performance-differentiated products and Volvo’s safety DNA, Volvo is considering making our lidar standard on all vehicles in the future, which would further enable and accelerate the adoption of our technology to several automotive partners. 
This, in turn, increases our ability to scale incorporation of our products into additional passenger vehicles relative to our competitors, which we believe is a significant advantage. With production expected to start with Volvo in 2022, we will have an industrialized, automotive-grade product ready to deploy and the ability to leverage existing capacity with an efficient use of capital to support our commercial partners globally. 
Commercial Trucking 
We work with a significant majority of self-driving truck start-ups and traditional truck OEMs. Our commercial partners greatly value the long perception range that our sensors enable while operating on highways. Our technology enables the detection of road debris such as tire remnants or stalled traffic at ranges greater than 250 meters, as well as motorcycles darting through traffic at highway speeds. We believe the short-range performance of the vast majority of lidar providers is insufficient against those and other scenarios and inadequate to provide the level of safety required by commercial trucking companies operating on public motorways. 
We work directly with our commercial partners to optimize our products for their applications. A few highlights of this optimization include our developments of unique scan patterns for maximized point density in specific areas of interest and models for sensor placement that minimize blind spots around the cab. Our commercial partners use between one to four lidar sensors per truck, and we expect that all will eventually integrate three or four if they move forward to series production. 
We enable our commercial trucking partners to consider three and four sensor configurations because of our expected unit economics. While the trucking market has less price sensitivity than the passenger vehicle market to support a multiple sensor configuration, it still benefits from the economies of scale achieved in the higher volume passenger vehicle market. Our commercial trucking development partners also appreciate that our passenger vehicle development comes with automotive-grade standards implemented in our product design and manufacturing processes. This enables our commercial trucking development partners to leverage our success with passenger vehicles and access the technology required to deploy much sooner than if they had worked with our competitors. We believe this is significant to them as the economic incentive for self-driving trucks is more compelling than for passenger vehicles since truckload carriers in North America and Western Europe aggressively compete for freight down to a difference of tens of dollars. Self-driving technology will enable truckload carriers to eliminate drivers on their terminal to terminal lanes and subsequently eliminate 25% to 30% of their costs for hauling freight. They will use that savings to win more desirable freight business. Adding to truckload carriers’ sense of urgency to deploy self-driving truck technologies is the chronic shortage of drivers. For these reasons, we believe self-driving trucks will start to operate on highways as early as 2023 and steadily ramp up through the remainder of the decade. 
Autonomy is a true economic enabler for the logistics market, including terminal to terminal, drayage and even last-mile delivery. The benefits of proactive safety discussed as part of our consumer vehicle products also apply to trucking. 
Robo-Taxi and Delivery Vehicle Market 
While robo-taxi and self-driving shuttle development primarily focus on low-speed urban environments today, their full value will only be met if they can also operate at higher speeds to expand their operating area, such as highways leading to airports. Our technology helps them achieve those goals by expanding this operating area to include roadways with speeds greater than 45 mph. Moreover, by using our perception software, our commercial partners can utilize their limited engineering resources more efficiently and enable them to focus on solving issues associated with vehicle system integration and driving in complex, urban environments. Our technology complements their work and will enable them to deploy their fleets sooner. 
We expect there will be a number of locally dedicated robo-taxi R&D fleets continuing to launch through the next decade, which will begin with human safety drivers monitoring operation at all times and then transition to no human monitor as the fleet gains confidence in the safety of the system. 
Adjacent Markets 
The on-road vehicle markets are what drive our product development decision-making, especially in sensor hardware development, but the need for nearly identical performance exists in other markets as well. These markets commonly cannot match the economies of scale that automotive markets offer, but together they represent strong business opportunities. Therefore, we take an opportunistic approach to the broader lidar and perception markets, with particular near-term focus on the following. 
•Smart Cities. We are working with our partners to integrate our sensors and perception software into existing solutions to make those solutions perform at high levels. Our technology enables those systems to detect and respond to vehicles at much greater ranges than legacy technology, and its perception software enables more reliable classification and prediction of objects within the area of interest. For example, cities will be able to reduce accidents at troublesome intersections and avoid expensive redesign projects, and tolling agencies can reduce the number of missed vehicles and increase their revenue yield. Many other applications benefit from our technologies’ superior performance, and we are working with partners to enable new benefits for their customers. 
•Aerospace & Defense. Aligned with our mission of enabling the autonomous movement of people and goods, we work with large aerospace/defense contractors on applications that extend off-road. While our products are used in many different applications, most involve enabling some form of autonomous drive capability. We anticipate entering into multi-year supply agreements with our defense contractor partners in this market to generate a significant number of sensor sales in the future. We also expect that most of our defense contractor partners will integrate our perception software into their solutions. 
There is a significant difference between a development platform project and automotive-grade production. Many lidar companies have created development products. These products are used for multiple applications, including environmental mapping for autonomous driving perception. Some of these development products began with huge spinning lidar sensors placed on top of vehicles that were ideal for viewing 360° around the vehicle, in order to better understand the challenges associated with autonomy and help solve those challenges. They were deployed in robo-taxi and autonomous trucking applications, and a myriad of off-road applications to scope the role of 3D sensing. While relatively successful to date at establishing incumbent positions in all applicable markets, almost none of these products have transitioned to automotive-qualification or military standardization specification, which is required for series production. Many lidar companies have elected to shift their focus from automotive to other adjacent markets due to the deficiencies in their technical approach to lidar or the sheer organizational difficulty and cost in delivering automotive-grade products. Many of those adjacent markets are looking to leverage scale and reuse from the automotive market, with the understanding that it is very difficult to replicate a potential market of approximately 100 million units per year (passenger vehicles and commercial vehicles combined worldwide). With a clear roadmap and a development platform that seamlessly transitions into the production platform, we believe we are well-positioned to establish the mass-scale market for lidar as the key markets’ leadership position. 
Our Products 
We believe we have established a dominant position in a crowded lidar market for three critical reasons: product, thought leadership, and deployment. Our products are designed and built from the ground up for the automotive market, and our performance exceeds those of our competitors. Our lidar and perception software forges a path for consumer 1550nm technology, which has been widely embraced as the long-range wavelength region necessary to widely deploy truly autonomous vehicles. We believe we are the only lidar company with deeply integrated hardware and software products, and this depth is supported by an extensive intellectual property portfolio of 93 issued patents, in addition to 84 pending or allowed patents as of February 2021. 
We believe our products meet or exceed the requirements to enable safe autonomy at all levels, and we have turned this capability into a position of thought leadership in the market. From small technology companies to global OEMs, our over 50 commercial partners look to us for guidance on how to specify, test, and integrate lidar into their products. Our broad technical competency spans hardware, software, and system safety disciplines. This leadership role often begins with our product as a reference sensor in validating lesser performing sensors, including other lidar, radar, and cameras. From this, we have been successful in converting to platform deployments as our roadmap to series production has become more immediate. 
Vehicle platform deployments determine the scope and design of a partner’s series production vehicle system, and are therefore our anchor for future growth. Sensor changes in these development platforms are not taken lightly by the industry, and the closer these test vehicles get to feature demonstration, the more difficult it will become to displace our technology. Our products have won platform development positions in most of the world’s top automakers and autonomous trucking programs, in both cases often displacing legacy lidar providers. Broad deployments in a host of different vehicles and countries provide us with a global fleet multiplier, which will unlock future capabilities as we seek to broaden automation capabilities. With a clear roadmap to an automotive-qualified product expected by 2022 as part of Volvo’s next generation consumer vehicles based on Volvo’s SPA2 platform, the rest of the market now has direct line of sight to our first wave of driver out-of-the loop vehicle features and services. Once partners scope their series production vehicle system based on their development platforms with us embedded, we believe there is a higher likelihood of successfully closing a design win for the series production. 
Our Iris lidar sensor integrated into Volvo SPA2 platform with expected production in 2022 
ADAS has commoditized the idea of vehicle safety, but has not delivered the full promise of this technology, as discussed further in the section entitled “Technology Comparison” located below. Therefore, a large opportunity exists to build on this foundation of vehicle features. We plan to use our market position and technology leadership to create a new class of vehicle features aimed at maximizing the safety impacts of high-performance sensory perception. Given more than 90% of motor vehicle accidents in the U.S. are due to driver perception or action failure, our proactive safety initiative addresses crash avoidance features instead of merely severity mitigation features. To support and accelerate the delivery of a complete lidar-based ADAS and Level 4 highway autonomy program, we are expanding our software team. This expansion began with the addition of former members of Samsung’s Munich-based DRVLINE platform team previously responsible for delivering ADAS functionality for its mobility enterprise. 
Whole-Products for Growth 
A whole-product is everything that is required to ensure that targeted end customers can fulfill their compelling reason to buy. For us, this means doing more than delivering the best possible lidar sensor. It means we will: 
•maintain sensing superiority through advanced sensor development; 
•provide actionable data through continual perception software refinement; and 
•drive vehicle feature delivery through internal and external investment. 
Sensing Superiority 
We have successfully delivered on our roadmap to date for lidar and software technologies to enable autonomy programs like those envisaged for Volvo’s SPA2 platform expected to begin in late 2022. Following nearly five years in stealth developing our core architecture, key components and filing associated patents, in 2017, we introduced a prototype product, known as Model G, which brought custom technologies together to demonstrate what was possible from long-range, high-resolution lidar. In 2018, we launched Hydra, our product for testing and development programs, and in mid-2019 we launched Iris, our commercial volume-production product. In January 2020, we launched our perception stack, which we believe will lead to smarter sensing over time. Our Hydra, Iris and other products are described in further detail below: 
Luminar’s Hydra lidar sensors are dynamically configurable dual-axis scan sensors that detect objects up to 500 meters away over a horizontal field of view of 120° and a software configurable vertical field of view of up to 30°. High point densities in excess of 200 points per square degree enable long-range detection, tracking, and classification over the whole field of view.
Hydra lidar sensors and electronic compute unit 
Luminar’s Iris lidar sensors leverage the same core technology components in Hydra, but Iris is refined to meet the size, weight, cost, power, and reliability requirements of automotive qualified series production. Iris features two fully custom integrated circuits – driving both laser transmitter and receiver. The sophistication of the Iris lidar data outputs comes from four generations of deployed integrated circuit design, and supports our ability to stay ahead of market demands for data. 
Currently commercialized vehicle autonomy technology only incorporates Level 1 and Level 2 ADAS, or partial automation made possible with cameras and radar, and enhanced by lidar. We expect to become a commercially viable long-range lidar for automotive applications in Level 3 through Level 5 of vehicle autonomy, including full highway autonomy and urban and suburban autonomous driving. We believe Iris will be an efficient, automotive-grade, and affordable solution for series-production programs starting production in 2022. 
Iris lidar sensor 
With camera-like resolution of more than 300 points per square degree and high data fidelity, Iris reliably sees where objects are and understands what they are, even at long distances and in inclement weather. Combined with ongoing software updates, Iris becomes more capable over time, unlocking the roadmap to autonomy and broadening driver assistance. 
Sensing More 
We selected lidar as our primary sensing architecture in part because it is an effective active sensor, meaning it has its own source of light (laser) that it emits to detect targets, rather than a passive sensor which depends on reflected sunlight to measure targets. When designed appropriately, the sensor can capture large amounts of information about the targets – well beyond three dimensions (3D). Even today, as it only scratches the surface of what we expect lidar can bring to autonomy, we provide more than a 3D scene. Through a pipeline of signal processing in each point-cloud point, common surfaces can be identified, moving objects can be better understood, and target reflectance provides grey scale contrast to the scene. All these pieces of information are called point attributes, and they feed perception algorithms that ultimately discern what the targets are within a scene. The more information perception algorithms are given, the faster and more reliable the results become. 
Looking forward, we are exploring ways to extract environmental information of things people can intuit, but machines must measure. For example, understanding air motion would allow software to estimate objects’ weight and assess the danger to vehicles. The optics and photonics community has produced countless capabilities like these for metrology applications. We are developing this deep understanding of what is possible with the market’s mobility needs to create products that deliver continually increasing value. 
Our Software 
If a vehicle is to take an action on the road (e.g., accelerate, brake or steer) without human control, or even override human control, it must have an understanding of the driving environment. This understanding is called perception. The requirements for perception, and subsequently for the sensors providing necessary information underlying it, ultimately come from questions the vehicle system needs to have answered continuously to execute driving maneuvers safely in the real world. These questions are the same ones the human brain must continually assess to drive: 
•Where is the road, how is it organized into lanes, and which is the proper lane? 
•What driving rules apply to these lanes (e.g., lane change permission, speed, direction, traffic type)? 
•How is the vehicle moving now (speed, direction)? 
•What obstacles and other fellow travelers are in or near the roadway? 
•Where are these external objects (which lane, sidewalk, etc.), and how are they moving? 
With a confident and continuous understanding of the driving environment from our perception software, routes can be planned, risks can be assessed and actions can be sent to the vehicle’s control system. We, working closely with our partners, expect to deliver this full vehicle system capability. 
Core Sensor Software: Our lidar sensors are highly configurable and capture valuable information extracted from the raw point-cloud to promote the development and performance of perception software. Therefore, core sensor software features help our commercial partners to integrate, control, and enrich the sensor data stream before perception processing. These features include: 
•Automatic sensor discovery to expedite system startup time; 
•Extrinsic calibration to automate multi-lidar geometrical alignment; 
•Proprietary middleware to streamline advanced user interaction with both our hardware and software; 
•Horizon tracking to automate region-of-interest scanning focused where it matters most, the road ahead; 
•Normal vector point attributes to associate common surfaces like drivable space quickly and accurately assess object headings without multiple frames; and 
•Velocity vector point attribute to provide both radial and crossing velocities, point-by-point within each frame. 
Sentinel Software Tools: In March 2021, we introduced our new software product suite, Sentinel. Sentinel is the first full-stack autonomous solution for series production and deeply integrates Zenseact’s OnePilot autonomous driving software solution alongside Luminar’s Iris, lidar, perception software and other components as a foundation, enabling every automaker to offer Highway Autonomy and Proactive Safety capabilities. We plan to sell Sentinel both as a complete “turn-key” software solution to our customers to enable Highway Autonomy or just with our Perception Software or other specific software capabilities. 
Perception Software: Our advanced perception software builds on the core sensor software features and transforms lidar point-cloud data into actionable information about the integrated vehicle (ego) and its environment. These features include: 
•Semantic Segmentation—Each measured point contains an object class attribute. This feature enables smart detection and tracking algorithms as well as intelligent vehicle reactions to different types of objects. 
•Instance detection and Tracking—Frame-level instance detection of objects, lane markings as well as road surfaces and free space combined with our highway-focused tracking algorithms provide reliable, safe and stable data for decision-making algorithms. 
•State Estimation—Continually predicting and correcting an object’s location, velocity, and orientation through lidar odometry, real-time mapping, and localization.
Our perception software detecting, tracking, and classifying vehicles, lanes, objects, and drivable free-space, up to 250 meter away, in real-time. 
We provide velocity point cloud attributes at both the point and object level. 
Our velocity attribute measures objects moving both laterally and longitudinally. 
High-level Vehicle Function Software 
•Highway Autonomy: In order to deliver highway autonomy to OEMs like Volvo, we leverage Zenseact and other ecosystem partners and strong internal understanding of the full autonomy system. Highway autonomy will enable exit to exit functionality that takes full responsibility of the driving task even if the driver does not resume control in edge case emergencies. Early roll outs will be in limited highways, in limited environmental conditions and broaden as validation activities ensure safe ODD expansion. This capability is meant to allow passenger vehicles and commercial trucks alike to take occupants out of the driving loop so that they can utilize their time on other tasks. Further, highway autonomy systems will leverage over the air updates allowing them to grow even safer over time and expand their ODD through the life of the vehicle. 
•Proactive Safety: Our proactive safety capabilities in development are expected to represent a new generation of vehicle safety, meant to enable accident avoidance instead of merely mitigating crash severity. It is expected to serve as a continuously monitoring system that assesses risk to the vehicle and recommends corrective actions and more importantly intercedes proactively when a crash is imminent. This feature utilizes our extended range of confident situational awareness to broaden the ODD of legacy ADAS features, new safety features, and driver out-of-the-loop autonomous features. 
Autonomy Compute: Our electronic compute unit (“ECU”) is designed to accelerate the development of perception systems. Raw point-cloud inputs via ethernet, from up to four lidar sensors, are sent through a pipeline of processing layers to provide automated field coverage, enriched point-clouds, and ultimately, the perception outputs required for fusion and path planning. 
Hydra currently features a reference ECU that can run the full software pipeline described below on four sensors covering 360° with under 40 watts of power consumption. The software pipeline is built modularly and is compute-hardware agnostic, allowing us to integrate algorithms into any OEM domain controller regardless of chip provider preference. 
Hydra electronic compute unit for testing and development programs 
Iris is an advanced lidar perception solution for series-production autonomy that we believe solves the fundamental problem of reliable, long-range sensory perception for real-world self-driving vehicles. From autonomous highway driving to full autonomy in urban areas, Iris is configurable with one or multiple perception enhanced lidar sensors to fit consumer and commercial application needs. It is an efficient, automotive-grade, and affordable solution for series-production programs starting production in 2022. In order to deliver Iris, and build beyond perception into vehicle functions, we plan to leverage partners in both processing chips and vehicle system controllers to deliver the hardware necessary to meet the performance and cost goals necessary to enable proactive safety and highway autonomy for broad adoption. 
Accelerating Delivery 
We intend to enable autonomy and invent next-generation safety through continually identifying gating technologies required for progress and creating paths to deliver innovation through both internal development and partnering. 
Looking beyond sensory perception into vehicle functions, the mission of proactive safety requires technologies to optimize driver engagement and take control of driving functions when necessary. Finally, we believe that, while vehicle connectivity will not reduce the need for on-vehicle sensory perception, there is value in collaborative perception from all vehicles. Allowing vehicles to effectively see around corners and through traffic is expected to increase collision avoidance by a substantial amount. Therefore, we will seek to continue to collaborate with industry participants as these connectivity systems emerge, ultimately giving each Luminar enabled vehicle the collective understanding of all Luminar enabled vehicles in the driving environment. 
Remaining Focused 
From the beginning, we have taken a whole product mindset to product development leading to growth beyond sensor development. Balancing this mindset, however, is our desire to accelerate the time to market of these whole-products. Therefore, we focus relentlessly on products aligned with our targeted markets, partners where possible, and innovate where necessary to best serve complete solutions to those markets. As a result, we offer no short range only lidar products due to existing camera, radar, and ultrasonic capabilities that adequately serve this demand in automotive. We do not dilute our portfolio in hopes of finding a niche – we have identified the root requirements for large scale applications and deliver products to make them successful as efficiently as possible. 
For us to continue winning series production contracts, great sensors and perception alone are not sufficient, as other technologies are required to deliver the expected whole-product (including other sensors, higher levels of software, electronics infrastructure, and compute). We have, therefore, constructed an ecosystem of partners to streamline both the vision for and delivery of whole vehicle system products. Healthy ecosystems for cameras, radar, and their associated perception exist to serve the automotive market, and supporting infrastructure exists to support current features such as electronic stability control and LKA. Computer hardware is evolving, and progress is required before achieving the cost and power targets for broad consumer vehicle adoption. However, the path to achieving these targets continues to develop as companies execute on platform development programs and scope their series production targets, driving large enough demand to justify development and tooling. 
Technology Comparison 
There are two primary methods to compare our technology with the market:
•How we perform against and complement entrenched, non-lidar sensing technologies currently in-use; and 
•How we perform against potential lidar competitors. 
Below is a discussion of today’s technology (ADAS) and the sensors that support it (camera, radar), followed by an explanation of lidar performance and specifically how our lidar fares within the competitive landscape. 
Legacy Sensing Technologies 
Current industry ADAS capabilities are enabled primarily by camera and/or radar sensing technologies. Data from both sensor types are commonly merged to provide the vehicle system with some understanding of its driving environment. These systems, however, fall short of delivering substantial safety gains. 
ADAS aims to assist the driver in identifying specific dangerous situations and acting on their behalf in certain cases. Currently, the most advanced ADAS will brake and steer the vehicle when the human driver does not respond, but the features do not consistently react to a dangerous situation ahead. Today’s ADAS works well under ideal circumstances – at low speed, in ideal weather conditions, and on a test track. However, in adverse environmental conditions, the performance sharply deteriorates. We believe that with our Proactive Safety solution, we can decrease the reported collisions occurrence rates by up to seven times. 
As we continue to evaluate available technologies for lidar and develop our roadmap to complete vehicle features, we seek to continue to actively monitor all other technologies, such as radar and camera sensing. Many of these technologies complement lidar (discussed below) and have pre-existing platform positions with automakers. 
Commodity Components in Automotive 
Camera. Cameras can be categorized into two important capabilities, monocular (2D, commonly referred to as mono cameras) and binocular (3D, commonly referred to as stereo cameras). Mono camera perception is the primary ADAS sensing component today and moving toward near complete adoption in new vehicles in Europe, the U.S., South Korea, and Japan. China also shows significant adoption increase, albeit far from standard equipment. It delivers a large set of perception capability which enables many functions that are widely offered to consumers: LKA; LCS; automatic high beam control; traffic sign recognition; and, in some cases, ACC. Mono cameras also support a wide range of ADAS safety cases whereby the detection and classification of objects enable crash mitigation. For instance, AEB for vehicles, cyclists, pedestrians, and animals is largely enabled by camera perception technology. The main benefit of mono cameras is their low cost. However, with this low cost comes limitation. Beyond performance degradation in poor environmental conditions, the distance measurement to an object is just an estimation based on the object scale and not a true measurement. This limits the mono cameras’ ability to robustly measure the distance and understand the trajectory of an object and, therefore, has limited ability to safely control the vehicle. 
To combat the range measurement deficiencies of mono cameras, some OEMs and Tier-1 suppliers have decided to develop stereo cameras which use two separate cameras, set apart by a particular distance, to deliver the same functions as mono cameras but with a much better depth estimation. While this works well at short range depth estimation, extending to longer ranges requires wide separations, sensitive optical alignment, and very high resolution – all things that eliminate the commodity pricing benefit of cameras. Furthermore, like mono cameras, stereo cameras are limited in inclement weather, and performance is heavily dependent on optical alignment and lighting. 
Radar. When it comes to ADAS technology, radar has been viewed as the pioneer. The first application of radar in passenger vehicles dates back to approximately 1998, where ACC was first offered to consumers. Adopted from military applications, long-range radar and mid-range radar were placed at the front of the vehicle to specifically detect lead car distance and speed. There have been many technological advancements in radar, but the functionality delivered is largely the same: a 
very accurate distance and speed measurement of objects, but little to no understanding of what they are, or precisely where they are horizontally or vertically. The volume driver of radar has been the AEB function as OEMs use camera and radar fusion to increase the robustness of their low-speed ADAS offerings and deliver NCAP 5-star vehicles that mitigate the severity of accidents. 
Radar is usable in nearly all weather and environmental conditions (except for heavy snow) and works at all times of the day. Given the benefits radar brings to fusion systems, its robustness and its cost (significant commoditization of radar has occurred in the past decade), it is likely to remain a staple for today’s ADAS systems and we see radar adoption growing towards near complete adoption by 2026, including surround sensing for functions such as blind spot detection, cross traffic alert, and lane-change merge assistance. 
Lidar wavelengths around 1,550 nanometers (such as ours) are approximately 2,000 times shorter than radar wavelengths (>3mm); this allows for resolution capabilities approaching that of cameras. Radar can theoretically achieve <1.0° resolution, but the device’s physical size must become very large in order to achieve this, and delivering <0.1° (like lidar) approaches physical impossibility. Therefore even “imaging radar” can, at best, only approach the performance of very low performance lidar, which does not unlock any new valuable features for the automotive industry. Furthermore, the maturity of these advanced radar technologies is less commercially mature than lidar and thus these advanced radar technologies may never find a price/value fit in the automotive industry until they become as low cost as today’s commodity radar. As such, there is minimal growth potential for radar technology in terms of added functionality. Rather, there is likely a market for 1550 nanometer lidar for replacement of forward-looking applications given the large perception capability gains that unlock next-generation features. 
Sensors to enable autonomy 
Given their performance in ADAS, there is little confidence that radar and cameras alone will enable autonomous driving, as evidenced by the vast majority of autonomous driving development efforts globally. There are many views about the difficulty of achieving fully autonomous driving and the sensing technology required to get there. However, with every fatal crash due to camera and/or radar perception failure in semi-automated vehicles in the market today, the need for better 3D sensing and processing becomes more apparent. Lidar has the potential to be that key sensor, and our lidar leads the way with proprietary technology and perception systems to unlock this next generation roadmap of vehicle features. 
Lidar Purpose and Requirements. We believe lidar is a necessary complement to existing cameras and radar in systems pursuing proactive safety and fully autonomous driving. High performance lidar combines the classification capabilities of cameras, the direct object distance measurement capability of radar, and adds a direct 3D drivable space assessment that neither method can deliver, and which is critical to AD. 
Intelligently combining these three sensing modalities provides high confidence perception in a broad set of operational domains, unlocking the next generation of vehicle safety. 
We believe a vehicle’s vision must be strong for all use cases-there is no compelling long-term use case for short-range lidar alone. 
These top-level requirements are met as a single operating mode, not just one at a time, by our lidar, which is a critical reason our partners see rapid progress after integration. The key, top-level requirements are: 
• Range 
• Resolution 
• Field-of-view 
• Fidelity 
• Frame rate. 
We believe that to provide long-term value through the necessary use cases, no single performance metric should be sacrificed for another. All are critical and must be met simultaneously. Additionally, safety and safe autonomy are not only needed during clear weather and good lighting conditions, but rather they must perform in all conditions a person would drive in, and hopefully more. Therefore, the sensing technology must perform in all weather and all lighting conditions and it must be uninfluenced by interference from the sun and other lidar signals that may be present in the environment. 
Top-Level Requirements: 
Iris lidar sensors are designed to meet all of these requirements in one streamlined package. Every aspect of our lidar was intended to be designed to safely meet the functional performance needed to unlock highway autonomy. The chart below shows a comparison of the technical design selections made by the rest of the market and how they fare compared to the technical design selections made by us based on our internal assessments. 
All data sourced from Luminar and other lidar company spec sheets and physics. 
The “Ranging” category captures how a sensor measures each pixel’s range, and the “Field Coverage” category captures how a sensor collects all those pixels from around the scene. While there are many differentiation points covered below, it is critical to focus on wavelength (light “color”) choice as it is a matter of eye safety. Near-visible infrared wavelengths, such as 905nm, are more hazardous to eyes than longer wavelengths because even though not visible, their energy is still focused onto the retina. 905nm is the most common lidar wavelength, and it is indeed very close to visible for humans (850nm light can commonly be seen as a dim red). Therefore, these sensors are severely limited in how much light they can safely send into the world for measurements. This is why we, very early on, committed to a longer wavelength lidar design – something that began as controversial and has become the market expectation for long-range lidar. 

Design Area | | Common Lidar Architectures | | Luminar 
 | | Lidar Architecture 
Wavelength | | 905nm | | 1550nm 
 | • Range limited by eye-safety | | • Low cost with single pixel Ingas 
 | • Resolution limited by eye-safety | | • Allows for long range, high resolution 
 | | | • Allows for deeper weather penetration 
Ranging | | FMCW | | Single-pulse time of flight 
 | • Range/Resolution limited by continuous wave measurement | | • Low complexity, low part-count 
 | • Costly due to high transceiver count | | • High rate measurements with high confidence 
 | | Single Photon Detection | | 
 | • Range/Resolution limited by continuous wave measurement | 
 | • Costly due to large, complex detector array | 
Scanning | | Flash | | Low-mass, encoded mirror scanning
 | • Range limited by eye-safety | | • Scanning of an isolated, field of view 
 | • Costly due to large, complex detector array | | • Low noise and rejection of uncontrolled light (sun, headlights, other lidar)
 | | Spinning 1D Array | | 
 | • Lifetime limited due to massive mechanical motor | 
 | • Software reliability limited by noise and artifacts | 
 | • Costly due to high alignment burden and component count | 
 | | MEMS | | 
 | • Range/Resolution limited by high noise | 
 | • Angular precision limited by fragile, non-micro scanner | 
 | • Software reliability limited by noise and artifacts | 
 | | Optical Phased Array | | 
 | • Range limited due to transmit loss | 
 | • Resolution limited due to poor beam control and quality | 
 | • Low reliability due to side-lobe illumination | 

Whether the design decision is based on achieving the lowest possible price or utilizing an existing technology or supplier, these selections have tradeoffs that impact performance and lessen the sensor’s usefulness in the vehicle market. Our analysis here is focused on what is required for vehicular safety and autonomy. Our lidar endeavors to minimize such tradeoffs and, through innovation, delivers a product to enable robust safety and true autonomy. 
Multiple Sensors and Fusion. Much of what we see in the vehicle market today is fusion of camera and radar, which typically addresses medium and high-speed applications at low levels of autonomy (hands-on, eyes-on). As radar provides a significantly more robust distance measurement than stereo cameras, the industry has generally elected to use mono camera technology and radar together with only a few customers continuing to use the stereo-camera technology. We see this as an interim technology as mono camera capability improves with increases in the number of pixels, and as lidar capability increases and pricing reaches a level that can be implemented affordably for all vehicle segments. These sensors work independently from one another, have different sensing modalities, so are not typically subject to the same failures, and work reasonably well in identifying obstacles and avoiding them. Ultrasonic sensors are another sensor type used for detection and ranging and usually used at low speed (less than 8 km per hour) and parking applications. These sensors are sometimes also fused with cameras to enable more automated parking functions. They are also used for blind spot detection functions, but the detection range is limited to no more than 10 meters, and radar is the more common sensor. 
As technology advances up the autonomy levels to allow hands-off and eyes-off driving, the requirement for sensing robustness, redundancy, precision, and accuracy become more and more critical. As noted in the “Legacy Sensing Technologies” section above, radar and camera alone can help mitigate accidents. The fusion of these two provides a reasonably good sensing solution, but typically only in ideal weather conditions. Today this solution set serves a majority of the market for ADAS to help achieve NCAP 5-star ratings globally and by 2026, will push the application of this technology toward standard equipment. From there, the ADAS market growth levels off, as does the effectiveness and benefit of just camera/radar fusion alone. Safety is essential at all times of the day, in many different weather and lighting conditions. To achieve the objective of zero fatalities, a lidar that meets all of the requirements outlined above is necessary. 
For highway autonomy, safety is paramount to allow the consumer to utilize more of their driving time to handle other tasks. A lidar that meets all of the requirements outlined here must be implemented for AEB (vehicles, pedestrians, cyclists, crossing cyclist, intersection, left turn across the path), head-on collision avoidance, and all other critical safety functions that should operate at lower and higher speeds to drive down the nearly 35,000 U.S. deaths per year still caused by auto accidents. 
Competition 
The market for lidar-enabled vehicle features, on and off road, is an emerging one with many potential applications in the development stage. As a result, we face competition for lidar hardware business from a range of companies seeking to have their products incorporated into these applications. We hold a strong position based on both product performance and maturity, but also in our ability to develop beyond the sensor itself into software functions. 
Although we believe that we are the only provider of lidar for automotive autonomy applications that achieves the industry’s requirements and perception capabilities to enable safe hand-off, eyes-off driving, we face potential competition from Tier 1 companies, and other technology companies. It will take these new emerging technologies a substantial period of time to achieve new levels of lidar capabilities. We believe many of our competitors offer more limited solutions for niche applications and are often non-automotive. In the meantime, our software development will differentiate our product offerings away from “lidar only” making it more difficult for lidar competitors to become broadly adopted. 
Some lidar competitors are currently selling solutions that offer lower levels of sensor performance in ADAS, a demand we do not see substantiated in the market due to low cost competition from camera and radar-based perception solutions for low levels of autonomy. While lidar competitors will continue to emerge and recede, our high performance lidar with a strong intellectual property portfolio and software products establish barriers to those who follow. 
Beyond automotive, the adjacent markets, including delivery bots and mapping, among others, for lidar are highly competitive. There are entrenched incumbents and competition, including from China, particularly on ultra-low cost products that are widely available. 
Within the automotive autonomy software space, the competitive landscape is still nascent and primarily focused on developing robo-taxi technologies rather than automotive-grade autonomy for highway applications. Their software technology generally depends on legacy sensing suites that are ubiquitous across the industry and lacking in performance capabilities to enable safe autonomy. 
We believe our technology and continuing innovation will support our position as a leader in advancing lidar technology in the market based on several market differentiators. 
Intellectual Property 
Our success and competitive advantage depend in part upon our ability to develop and protect our core technology and intellectual property. We own a portfolio of intellectual property, including patents and registered trademarks, confidential technical information, and expertise in the development of lidar technology and software for autonomous vehicles. 
We have filed patent and trademark applications in order to further secure these rights and strengthen our ability to defend against third parties who may infringe on our rights. We also rely on trade secrets, design and manufacturing know-how, continuing technological innovations, and licensing and exclusivity opportunities to maintain and improve our competitive position. Additionally, we protect our proprietary rights through agreements with our commercial partners, supply-chain vendors, employees, and consultants, as well as close monitoring of the developments and products in the industry. 
As of February 2021, we owned 93 issued patents and have 84 pending or allowed patent applications, including U.S. and foreign. In addition, we have three registered U.S. trademarks, 16 registered foreign trademarks and five pending trademark applications. Our patents and patent applications cover a broad range of system level and component level aspects of our key technology including, among other things, lidar system, laser, scanner, receiver, and perception technology. 
Scalable Manufacturing Process 
We have internally developed the manufacturing and testing processes, including capturing any related intellectual property, necessary to develop our products. Building or designing critical components in-house rather than using off-the-shelf commodity components provides for protectable and sustainable technology differentiation from lidar competitors or alternative technologies. We believe significant barriers to entry for automotive lidar are the processes and know-how to manufacture a compact and intricate sensing product. Our manufacturing processes and knowledge are a key differentiator for us in the market. The product concept and design-for-manufacturing were considered as part of the product development process from the beginning of our product development. 
Instead of relying on external resources to develop our product solutions, we have developed these skills and capabilities in-house, leveraging key hires’ expertise in the industry and establishing an advanced engineering team. We have developed solutions for optical alignment, high precision placement of silicon within the required tolerance to deliver the specified performance, and worked with suppliers on end-of-line testing for a cost-effective long-range detection system. 
Iris Product Industrialization and Manufacturing Globalization 
Iris is the third commercial generation lidar platform to be developed by us (after Model G and Hydra). In Iris’s development, we have leveraged two prior cycles of learning and shipping for faster time-to-market as it is being developed and prototyped in the same facilities by the same teams as its predecessor generations. Both the operations and engineering teams are co-located to ensure that our manufacturing and engineering teams work hand-in-hand. 
We expect Iris will first launch as a North American-built product with the first sensor assembly expected to be in an International Automotive Task Force (“IATF”)-certified plant in Mexico at our anticipated lead contract manufacturer. We expect the supply chain will include critical technology suppliers from around the world. 
This anticipated lead contract manufacturer also has IATF-certified locations in Europe and Asia. These factories would be brought online as volume dictates, and as we achieve scale and supplier localization in specific regions to best support our global commercial partners. 
Material Agreements 
Volvo Series Production Contract 
In March 2020, we entered into a series production contract with Volvo Car Corporation (“Volvo”) to equip our products into its next-generation vehicle platform, called SPA2, for which its future consumer vehicle models will be based. The intent of the program is primarily to enable highway autonomous drive capability as an option on production consumer vehicles, with series production expected to start in 2022. Additionally, the program presents an opportunity to simultaneously enable next-generation proactive safety systems in a more widespread capacity at lower cost than autonomous drive upgrades. 
Pursuant to the agreement, we are currently collaborating with Volvo in an agile framework that is relatively novel to the automotive industry and traditionally associated with software development. This agile method allows for close interactions between our and Volvo’s teams to produce high quality work products on faster paced timelines than is traditionally associated with automotive companies. 
Under the agreement, Volvo and we have each agreed to make certain relevant investments to enable the greatest possible success of the program. As part of this, Volvo is currently compensating us for certain work products as the program progresses to Start of Production (SOP) in 2022. 
The agreement contains certain minimum volume targets for several geographies for specified periods for specific vehicle models. The production volumes will ultimately be highly dependent on numerous factors including end consumer feature take rate, larger automotive industry demand, and the speed at which we are able to scale to meet such demand, all of which are not binding for either party. 
Following an automotive grade production audit and qualification of our advanced manufacturing factory in Orlando, Florida, under the agreement Volvo has certified us to produce lidar sensors for them out of our internal facility, with the opportunity to outsource series production to a third party pending Volvo’s automotive quality certification. 
The agreement is a long-term, multi-year contract that terminates fifteen years following the end of Volvo’s series production involving our products. Volvo or we may terminate the agreement for cause under certain conditions, including if we undergo a change of control, at an earlier time. 
Research and Development 
Our research and development activities occur in Orlando, Palo Alto, and Colorado Springs in the United States, and in Munich, Germany. Orlando is primarily focused on developing sensor hardware, firmware, and controllers, and Palo Alto 
develops perception software. We are also expanding software development with a new team in Germany. The Colorado Springs location creates the custom ASIC chips used in our lidar sensors. 
Our research and development team is responsible for creating new technology and expanding lidar and perception software functionality. The team also designs the physical product, ensures it is designed for manufacturability and performs testing. The team also partners with our operations and supply chain functions to develop scalable commercial and reliable manufacturing processes and direct production material procurement. 
Sales and Marketing 
We take an insight-driven, account-based marketing approach to build and expand our relationships with commercial partners. We collect feedback directly from commercial partners to garner insights that help drive the business and product. We also work with analysts and higher education institutions to conduct studies, test and validate technology performance, providing key proof points for commercial partners considering our products. In parallel, marketing and communications drive our brand equity and narrative through ongoing announcements, campaigns, events, speaking opportunities, and public relations efforts. 
Government Regulation 
At both the federal and state level, the U.S. has provided a positive legal environment to permit safe testing and development of autonomous functionality. We do not anticipate any near-term federal standards that would impede the foreseeable deployments of our lidar technology. Some states, however, particularly California and New York, still enforce certain operational or registration requirements for certain autonomous functions. We believe such hurdles will be removed as state regulators gain better experience with the technology. U.S. federal regulations, however, remain largely permissive of deployments of higher levels of safe and responsible autonomous functionality. 
Foreign markets such as the EU and China also continue to develop their respective standards to define deployment requirements for higher levels of autonomy. Given the intense work in these areas, we expect a workable path forward in the near-term. 
As vehicles equipped with our sensors are deployed on public roads, we will be subject to the legal and regulatory authorities of principally the NHTSA. The obligations of motor vehicle equipment manufacturers include regular reporting under the Transportation Recall Enhancement, Accountability and Documentation Act process as well as strict recall and reporting requirements for any defects related to highway safety or any non-compliance with a Federal Motor Vehicle Safety Standard. Similar such reporting and recall requirements exist in foreign markets. As the development of federal, state and foreign legal frameworks around autonomous vehicles continue to evolve, we may be subject to additional regulatory schemes. 
As a lidar technology company, we are subject to the Electronic Product Radiation Control Provisions of the Federal Food, Drug, and Cosmetic Act. These requirements are enforced by the U.S. Food and Drug Administration (“FDA”). Electronic product radiation includes laser technology. Regulations governing these products are intended to protect the public from hazardous or unnecessary exposure. Manufacturers are required to certify in product labeling and reports to the FDA that their products comply with applicable performance standards as well as maintain manufacturing, testing, and distribution records for their products. 
Similarly, as a global company deploying cutting-edge technology, we are also subject to trade, customs product classification and sourcing regulations. Finally, our operations are subject to various federal, state and local laws and regulations governing the occupational health and safety of our employees and wage regulations. We are subject to the requirements of the federal Occupational Safety and Health Act, as amended, and comparable state laws that protect and regulate employee health and safety. 
Like all companies operating in similar industries, we are subject to environmental regulation, including water use; air emissions; use of recycled materials; energy sources; the storage, handling, treatment, transportation and disposal of hazardous materials; and the remediation of environmental contamination. Compliance with these rules may include permits, licenses and inspections of our facilities and products. 
Employees 
We have always prioritized the team’s importance, with values-based hiring that encompasses competency, ingenuity, and culture. Through multiple growth phases, we have drawn talent and leadership from the automotive, aerospace, and consumer electronics industries to achieve its vision. As of December 31, 2020, excluding contractors, we had 368 full-time employees and four part-time employees worldwide consisting of 17 in Europe, 81 in California, 243 in Florida, 24 in Colorado and seven in other locations. None of our employees are represented by a labor union, and we consider our employee relations to be in good standing. To date, we have not experienced any work stoppages. 
Our human capital resources objectives include, as applicable, identifying, recruiting, retaining, incentivizing and integrating our existing and new employees, advisors and consultants. The principal purposes of our equity and cash incentive plans are to attract, retain and reward personnel through the granting of stock-based and cash-based compensation awards, in order to increase stockholder value and the success of our company by motivating such individuals to perform to the best of their abilities and achieve our objectives. 
Facilities 
Our corporate headquarters is located in Orlando, Florida, where we lease a complex of three buildings with 120,716 square feet pursuant to leases that expire between October 2022 and September 2024. The Orlando facilities contain manufacturing, engineering, research and development, and administrative functions. We also lease 36,419 square feet of office and engineering space in two facilities in Palo Alto, California and 12,900 square feet of office and engineering space in a facility in Colorado Springs, Colorado. The Company believes its existing facilities are adequate for its current requirements. 
Legal Proceedings 
From time to time, we may become involved in actions, claims, suits, and other legal proceedings arising in the ordinary course of our business, including assertions by third parties relating to intellectual property infringement, breaches of contract or warranties or employment-related matters. We are not currently a party to any actions, claims, suits or other legal proceedings the outcome of which, if determined adversely to us, would individually or in the aggregate have a material adverse effect on our business, financial condition, and results of operations. 
Corporate Social Responsibilities and Sustainability 
We are committed to active and responsible corporate citizenship. In the second quarter of 2020, we formalized our Corporate Social Responsibility (“CSR”) program to streamline the existing compliance and social justice activities within the company. The CSR program is divided into seven elements (diversity and inclusion; human resources; finance/accounting; responsible sourcing; environmental, health and safety; trade compliance; and business ethics), each spearheaded by company leaders and subject matter experts in their respective areas. The CSR team will act to support, advise, and provide mutual oversight for each element and drive reasonable and measurable advancement. 
